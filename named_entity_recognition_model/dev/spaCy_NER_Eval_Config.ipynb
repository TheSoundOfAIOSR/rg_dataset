{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Config.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of:\n",
        "    * Enormous dataset size\n",
        "    * Transformer based architecture involving GPU usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0JtYDT_Dx7S"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBp28TAJmhB"
      },
      "source": [
        "## Install torch\n",
        "\n",
        "* Install torch specifc to the Google Colab's CUDA version\n",
        "* CUDA version 11.1 works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LAGZmbvE0VQ"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WzjtepqIlx2"
      },
      "source": [
        "## Extract Project files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4cI8yUIscq"
      },
      "source": [
        "!unzip /content/project.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZDk28NRC_s8"
      },
      "source": [
        "## Pre-process and save to json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wph-DdQFKPnO"
      },
      "source": [
        "!unzip /content/augmented_dataset_2021-06-21.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjtHcwhDMYD"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "\n",
        "TRAIN_DATA_PATH = \"./augmented_dataset_2021-06-21/train.csv\"\n",
        "TEST_CONTENT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_content.csv\"\n",
        "TEST_CONTEXT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_context.csv\"\n",
        "TEST_UNSEEN = \"./augmented_dataset_2021-06-21/test_unseen.csv\"\n",
        "\n",
        "def load_cleaned_data(data_path):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    col_names = ['text', 'entities']\n",
        "\n",
        "    data = pd.read_csv(data_path, names=col_names, usecols=[0, 1])\n",
        "    # print(data.head())\n",
        "    entity_list = data.entities.to_list()\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"tokens\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        annot_list = []\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # print(index)\n",
        "        # print(data['text'][index].lower())\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "\n",
        "                # Remove any leading or beginning blank space\n",
        "                word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "                start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "                start_index = int(start_index + 0)\n",
        "                end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "                # Incase word not found in the sentence\n",
        "                if start_index == -1:\n",
        "                    print(\"-1 error\")\n",
        "                    print(data['text'][index])\n",
        "                    break\n",
        "\n",
        "                both_present = lambda: (start_index, end_index) in indices_list\n",
        "                start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "                end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "                left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "                def right_blank():\n",
        "                    # return true if there is no blank space after the end_index,\n",
        "                    # as long as end_index is not at the end of the sentence\n",
        "                    if len(data['text'][index].lower()) != end_index:\n",
        "                        return data['text'][index][end_index] != \" \"\n",
        "                \n",
        "                # Check if this start_index and/or end_index is already in the list:\n",
        "                # (To prevent overlapping with already tagged words)\n",
        "                flag = 0\n",
        "                while True:\n",
        "                    if (start_index == -1 or end_index == -1):\n",
        "                        flag = 1\n",
        "                        break\n",
        "                    if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                    \n",
        "                        start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                           start=end_index + 1).astype(\n",
        "                            numpy.int64)\n",
        "                        start_index = int(start_index + 0)\n",
        "                        end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "                    else:\n",
        "                        indices_list.append((start_index, end_index))\n",
        "                        break\n",
        "                \n",
        "                if (flag == 1):\n",
        "                    # Don't bother checking rest of the current sentence\n",
        "                    break\n",
        "                \n",
        "                annot_list.append((start_index, end_index, word_pair_list[1]))\n",
        "        # print(data['text'][index].lower())\n",
        "        # print(annot_list)\n",
        "        DATA.append({\"text\": data['text'][index].lower(), \"entities\": annot_list})\n",
        "\n",
        "    # save_list_to_txt(DATA)\n",
        "    return DATA\n",
        "\n",
        "\n",
        "TRAIN_DATA = load_cleaned_data(TRAIN_DATA_PATH)\n",
        "TEST_CONTENT = load_cleaned_data(TEST_CONTENT_DATA_PATH)\n",
        "TEST_CONTEXT = load_cleaned_data(TEST_CONTEXT_DATA_PATH)\n",
        "UNSEEN_DATA = load_cleaned_data(TEST_UNSEEN)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QamFbSfcXJT"
      },
      "source": [
        "import json\n",
        "if not os.path.exists(\"assets\"):\n",
        "        os.makedirs(\"assets\")\n",
        "\n",
        "\n",
        "with open('assets/TRAIN_DATA.jsonl', 'w') as f:\n",
        "    for entry in TRAIN_DATA:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "with open('assets/TEST_CONTENT.jsonl', 'w') as f:\n",
        "    for entry in TEST_CONTENT:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "with open('assets/TEST_CONTEXT.jsonl', 'w') as f:\n",
        "    for entry in TEST_CONTEXT:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "with open('assets/UNSEEN_DATA.jsonl', 'w') as f:\n",
        "    for entry in UNSEEN_DATA:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsTfX-vqBd3"
      },
      "source": [
        "!zip -r /content/assets.zip /content/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQf0WHMtsGCy",
        "outputId": "4d30f7e3-5361-4e6f-84a6-ad652bed2590"
      },
      "source": [
        "!unzip /content/assets.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/assets.zip\n",
            "   creating: assets/\n",
            "  inflating: assets/TEST_CONTENT.jsonl  \n",
            "  inflating: assets/TEST_CONTEXT.jsonl  \n",
            "  inflating: assets/TRAIN_DATA.jsonl  \n",
            "  inflating: assets/UNSEEN_DATA.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jvWOqtpN9rT"
      },
      "source": [
        "## Convert the data to spaCy's binary format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lBf5MoxOC3D",
        "outputId": "edba645f-1bb9-4d8a-dddb-1c60ec4c1b0d"
      },
      "source": [
        "!python -m spacy project run preprocess"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 15:24:54.441621: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/preprocess.py assets/TRAIN_DATA.jsonl corpus/TRAIN_DATA.spacy\n",
            "2021-06-25 15:24:58.646674: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/preprocess.py\", line 31, in <module>\n",
            "    typer.run(main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 859, in run\n",
            "    app()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 214, in __call__\n",
            "    return get_command(self)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 497, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"scripts/preprocess.py\", line 16, in main\n",
            "    if eg[\"answer\"] != \"accept\":\n",
            "KeyError: 'answer'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xds9LuitNqch"
      },
      "source": [
        "## Check the config file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVrMv3h_NgNa",
        "outputId": "15b8d90b-fd11-4b01-d337-0b58339582fd"
      },
      "source": [
        "!python -m spacy debug data configs/config.cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 11:17:13.327521: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "Downloading: 100% 481/481 [00:00<00:00, 371kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 3.52MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 2.70MB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 4.07MB/s]\n",
            "Downloading: 100% 501M/501M [00:13<00:00, 37.2MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: en\n",
            "Training pipeline: transformer, ner\n",
            "1735 training docs\n",
            "1735 evaluation docs\n",
            "\u001b[38;5;3m⚠ 1735 training examples also in evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (1735)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 80451 total word(s) in the data (8100 unique)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 8 misaligned tokens in the training data\u001b[0m\n",
            "\u001b[38;5;3m⚠ 8 misaligned tokens in the dev data\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with punctuation\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 4 warnings\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9jpo3BOOmY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqCmp12p6Pfa",
        "outputId": "3cfc37cc-07fb-40fa-e871-680ac644c2ad"
      },
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcH0mZT5OQ_K",
        "outputId": "cb83f46a-da0d-43bc-8a5e-6a2a640017be"
      },
      "source": [
        "# !python -m spacy project run train\n",
        "!python -m spacy train configs/config.cfg --output training/ --paths.train corpus/fashion_brands_training.spacy --paths.dev corpus/fashion_brands_eval.spacy --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 11:29:50.403870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-06-25 11:30:06,473] [INFO] Set up nlp object from config\n",
            "[2021-06-25 11:30:06,487] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2021-06-25 11:30:06,492] [INFO] Created vocabulary\n",
            "[2021-06-25 11:30:06,492] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2021-06-25 11:31:01,221] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2891.67     78.75    0.11    0.06    0.42    0.00\n",
            "  0     200       21937.78   2406.90    0.71    2.33    0.42    0.01\n",
            "  0     400         247.67    511.16   63.00   55.84   72.27    0.63\n",
            "  1     600         184.20    378.35   62.22   59.92   64.71    0.62\n",
            "  1     800         174.02    379.50   68.19   62.37   75.21    0.68\n",
            "  2    1000         160.75    313.37   66.42   59.47   75.21    0.66\n",
            "  3    1200         197.47    415.16   69.53   62.63   78.15    0.70\n",
            "  4    1400         433.49    309.63   72.76   64.40   83.61    0.73\n",
            "  5    1600          65.86    266.12   67.80   61.43   75.63    0.68\n",
            "  7    1800          52.17    264.47   72.41   67.77   77.73    0.72\n",
            "  9    2000          52.85    311.82   71.91   64.86   80.67    0.72\n",
            " 12    2200          25.40    301.36   71.02   65.37   77.73    0.71\n",
            " 15    2400          77.64    439.52   75.71   79.00   72.69    0.76\n",
            " 18    2600          27.84    376.59   76.35   75.41   77.31    0.76\n",
            " 22    2800           3.54    321.35   75.67   69.10   83.61    0.76\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "RuntimeError('CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB\n",
            "total capacity; 9.88 GiB already allocated; 67.81 MiB free; 10.51 GiB reserved\n",
            "in total by PyTorch)')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 69, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 497, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 59, in train_cli\n",
            "    train(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 115, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 98, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 196, in train_while_improving\n",
            "    subbatch, drop=dropout, losses=losses, sgd=False, exclude=exclude\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1112, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/pipeline_component.py\", line 288, in update\n",
            "    trf_full, bp_trf_full = self.model.begin_update(docs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 309, in begin_update\n",
            "    return self._func(self, X, is_train=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/layers/transformer_model.py\", line 142, in forward\n",
            "    tensors, bp_tensors = transformer(wordpieces, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 291, in __call__\n",
            "    return self._func(self, X, is_train=is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/layers/pytorchwrapper.py\", line 80, in forward\n",
            "    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 27, in __call__\n",
            "    return self.begin_update(inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 49, in begin_update\n",
            "    output = self._model(*inputs.args, **inputs.kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 825, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 515, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 400, in forward\n",
            "    past_key_value=self_attn_past_key_value,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 330, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 253, in forward\n",
            "    attention_probs = self.dropout(attention_probs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1168, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 11.17 GiB total capacity; 9.88 GiB already allocated; 67.81 MiB free; 10.51 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWXqLUqCWjv"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8_FVJOfCQRx",
        "outputId": "50d392ab-d79b-4d74-9301-04c4d44a352b"
      },
      "source": [
        "# !python -m spacy project run evaluate\n",
        "!python -m spacy evaluate training/model-best corpus/fashion_brands_eval.spacy --output training/metrics.json --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-25 12:16:42.570823: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     99.99\n",
            "NER P   75.41\n",
            "NER R   77.31\n",
            "NER F   76.35\n",
            "SPEED   1619 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                    P       R       F\n",
            "FASHION_BRAND   75.41   77.31   76.35\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to training/metrics.json\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "# !unzip /content/saved_model.zip\n",
        "# !zip -r /content/data.zip /content/data\n",
        "!zip -r /content/img.zip /content/img\n",
        "# !zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}