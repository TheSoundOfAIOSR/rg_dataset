{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of enormous dataset size\n",
        "\n",
        "* Link to the dataset: https://drive.google.com/file/d/1qdjtMgGyafPNN_x2EANkcGLz5S-SB8mG/view?usp=sharing\n",
        "\n",
        "* The dataset is of size 2.6 GB, and contains more than 2M sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gwOh7y7NQ6"
      },
      "source": [
        "## Mount Google Drive to get acccess to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQdvZLkhFHs7",
        "outputId": "81df4ed1-05eb-4aa1-ee7f-dfa7f595267b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud_az1oZnELb"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7I12l76nCEv"
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "ITERATIONS = 5\n",
        "DROPOUT = 0.1\n",
        "LEARN_RATE = 0.001\n",
        "\n",
        "DATA_PATH = \"./data_filter_csv.csv\"\n",
        "\n",
        "# Path to the 2.6 GB augmented data\n",
        "BIG_DATA_PATH = \"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset.csv\"\n",
        "\n",
        "\n",
        "def load_cleaned_data(data_path=BIG_DATA_PATH):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    col_names = ['text', 'entities']\n",
        "\n",
        "    data = pd.read_csv(data_path, names=col_names)\n",
        "    # print(data.head())\n",
        "    entity_list = data.entities.to_list()\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"tokens\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        annot_list = []\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # print(index)\n",
        "        # print(data['text'][index].lower())\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "\n",
        "                # Remove any leading or beginning blank space\n",
        "                word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "                start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "                start_index = start_index + 0\n",
        "                end_index = start_index + len(word_pair_list[0])\n",
        "\n",
        "                # Incase word not found in the sentence\n",
        "                if start_index == -1:\n",
        "                    print(\"-1 error\")\n",
        "                    print(data['text'][index])\n",
        "                    break\n",
        "\n",
        "                both_present = lambda: (start_index, end_index) in indices_list\n",
        "                start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "                end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "                left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "                def right_blank():\n",
        "                    # return true if there is no blank space after the end_index,\n",
        "                    # as long as end_index is not at the end of the sentence\n",
        "                    if len(data['text'][index].lower()) != end_index:\n",
        "                        return data['text'][index][end_index] != \" \"\n",
        "                \n",
        "                # Check if this start_index and/or end_index is already in the list:\n",
        "                # (To prevent overlapping with already tagged words)\n",
        "                flag = 0\n",
        "                while True:\n",
        "                    if (start_index == -1 or end_index == -1):\n",
        "                        flag = 1\n",
        "                        break\n",
        "                    if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                    \n",
        "                        start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                           start=end_index + 1).astype(\n",
        "                            numpy.int64)\n",
        "                        start_index = start_index + 0\n",
        "                        end_index = start_index + len(word_pair_list[0])\n",
        "\n",
        "                    else:\n",
        "                        indices_list.append((start_index, end_index))\n",
        "                        break\n",
        "                \n",
        "                if (flag == 1):\n",
        "                    # Don't bother checking rest of the current sentence\n",
        "                    break\n",
        "                \n",
        "                annot_list.append((start_index, end_index, word_pair_list[1]))\n",
        "        # print(data['text'][index].lower())\n",
        "        # print(annot_list)\n",
        "        DATA.append((data['text'][index].lower(), {\"entities\": annot_list}))\n",
        "\n",
        "    # save_list_to_txt(DATA)\n",
        "    return DATA\n",
        "\n",
        "\n",
        "def save_list_to_txt(data, keyword):\n",
        "    with open(keyword + \".txt\", 'w') as f:\n",
        "        for item in data:\n",
        "            f.write(\"%s\\n\" % str(item))\n",
        "\n",
        "\n",
        "def save_list_to_pickle(list, name):\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    with open('data/' + name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(list, f)\n",
        "\n",
        "\n",
        "def load_list_from_pickle(filename):\n",
        "    with open(\"data/\" + filename + '.pkl', 'rb') as f:\n",
        "        list = pickle.load(f)\n",
        "    return list\n",
        "\n",
        "\n",
        "def split_data(DATA):\n",
        "    random.shuffle(DATA)\n",
        "\n",
        "    # Randomly pull out 10 % segments of DATA for test + eval\n",
        "    test_length = math.floor((10 / 100) * len(DATA))\n",
        "    TEST = DATA[:test_length]\n",
        "\n",
        "    random.shuffle(TEST)\n",
        "    # Randomly pull out 50 % segments of TEST_DATA for EVAL_DATA\n",
        "    eval_length = math.floor((50 / 100) * len(TEST))\n",
        "    EVAL_DATA = TEST[:eval_length]\n",
        "    TEST_DATA = TEST[eval_length:len(TEST)]\n",
        "\n",
        "    # Time consuming for our BIG data\n",
        "    # TRAIN_DATA = [x for x in DATA if x not in TEST]\n",
        "\n",
        "    TRAIN_DATA = DATA[test_length:len(DATA)]\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"\\nTotal sentences: \", len(DATA))\n",
        "    print(\"Length of train data: \", len(TRAIN_DATA))\n",
        "    print(\"Length of evaluation data: \", len(EVAL_DATA))\n",
        "    print(\"Length of test data: \", len(TEST_DATA))\n",
        "\n",
        "    TEST.clear()\n",
        "\n",
        "    return TRAIN_DATA, EVAL_DATA, TEST_DATA\n",
        "\n",
        "\n",
        "def plot_graph(title, keyword, precision=None, recall=None, fscore=None):\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "    legend_to_show = ()\n",
        "    if precision is not None:\n",
        "        plt.plot(x, precision, color='red', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='red', markersize=2)\n",
        "        legend_to_show += (\"precision\",)\n",
        "    if recall is not None:\n",
        "        plt.plot(x, recall, color='blue', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='blue', markersize=2)\n",
        "        legend_to_show += (\"recall\",)\n",
        "    if fscore is not None:\n",
        "        plt.plot(x, fscore, color='green', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='green', markersize=2)\n",
        "        legend_to_show += (\"fscore\",)\n",
        "    plt.gca().legend(legend_to_show, loc='best')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    plt.title(title + \" PRF Scores [\" + keyword + \"]\")\n",
        "\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"img\"):\n",
        "        os.makedirs(\"img\")\n",
        "\n",
        "    plt.savefig(\"img/plot_\" + title + \"_\" + keyword + \".png\", format=\"png\", dpi=my_dpi)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "def draw_prf_graph(train_scores, keyword=\"\", overall=True, instr=True, qlty=True, edge=True):\n",
        "    precision = []\n",
        "    recall = []\n",
        "    fscore = []\n",
        "\n",
        "    qlty_p = []\n",
        "    qlty_r = []\n",
        "    qlty_f = []\n",
        "\n",
        "    instr_p = []\n",
        "    instr_r = []\n",
        "    instr_f = []\n",
        "\n",
        "    edge_p = []\n",
        "    edge_r = []\n",
        "    edge_f = []\n",
        "\n",
        "    # Extract P, R, F from train_score\n",
        "    for i, train_score in enumerate(train_scores):\n",
        "        for key, cat in train_score.items():\n",
        "            if key == \"ents_p\": precision.append(cat)\n",
        "            if key == \"ents_r\": recall.append(cat)\n",
        "            if key == \"ents_f\": fscore.append(cat)\n",
        "            if key == \"ents_per_type\":\n",
        "                for attribute, value in cat.items():\n",
        "                    if attribute == \"QLTY\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": qlty_p.append(sc)\n",
        "                            if k == \"r\": qlty_r.append(sc)\n",
        "                            if k == \"f\": qlty_f.append(sc)\n",
        "                    if attribute == \"INSTR\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": instr_p.append(sc)\n",
        "                            if k == \"r\": instr_r.append(sc)\n",
        "                            if k == \"f\": instr_f.append(sc)\n",
        "                    if attribute == \"EDGE\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": edge_p.append(sc)\n",
        "                            if k == \"r\": edge_r.append(sc)\n",
        "                            if k == \"f\": edge_f.append(sc)\n",
        "\n",
        "    if overall is True:\n",
        "        plot_graph(title=keyword, keyword=\"overall\", precision=precision, recall=recall,\n",
        "                   fscore=fscore)\n",
        "    if qlty is True:\n",
        "        plot_graph(title=keyword, keyword=\"qlty\", precision=qlty_p, recall=qlty_r, fscore=qlty_f)\n",
        "    if instr is True:\n",
        "        plot_graph(title=keyword, keyword=\"instr\", precision=instr_p, recall=instr_r,\n",
        "                   fscore=instr_f)\n",
        "    if edge is True:\n",
        "        plot_graph(title=keyword, keyword=\"edge\", precision=edge_p, recall=edge_r, fscore=edge_f)\n",
        "\n",
        "\n",
        "def draw_train_eval_compare_graph(train_scores, eval_scores):\n",
        "    train_fscore = []\n",
        "    eval_fscore = []\n",
        "\n",
        "    for i, train_score in enumerate(train_scores):\n",
        "        for key, cat in train_score.items():\n",
        "            if key == \"ents_f\": train_fscore.append(cat)\n",
        "\n",
        "    for i, eval_score in enumerate(eval_scores):\n",
        "        for key, cat in eval_score.items():\n",
        "            if key == \"ents_f\": eval_fscore.append(cat)\n",
        "\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "\n",
        "    poly_order = 4\n",
        "\n",
        "    plt.plot(x, train_fscore, color='red', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='red', markersize=2)\n",
        "    train_reg_line = np.polyfit(np.array(x), np.array(train_fscore), poly_order)\n",
        "    p = np.poly1d(train_reg_line)\n",
        "    plt.plot(x, p(x), color='red', linestyle='--', linewidth=0.6,\n",
        "             marker='o', markerfacecolor='red', markersize=1, label='_nolegend_')\n",
        "\n",
        "    plt.plot(x, eval_fscore, color='blue', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='blue', markersize=2)\n",
        "    eval_reg_line = np.polyfit(np.array(x), np.array(eval_fscore), poly_order)\n",
        "    p = np.poly1d(eval_reg_line)\n",
        "    plt.plot(x, p(x), color='blue', linestyle='--', linewidth=0.6,\n",
        "             marker='o', markerfacecolor='blue', markersize=1, label='_nolegend_')\n",
        "\n",
        "    plt.gca().legend((\"train\", \"eval\"), loc='best')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(\"F-Score vs Epochs\")\n",
        "    plt.ylim(0.00, 1.00)\n",
        "    plt.savefig(\"img/plot_fscore_train_vs_eval.png\", format=\"png\", dpi=my_dpi)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_loss_graph(losses, title):\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "    plt.plot(x, losses, color='blue', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='green', markersize=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"img\"):\n",
        "        os.makedirs(\"img\")\n",
        "\n",
        "    plt.savefig(\"img/plot_loss_training\" + \".png\", format=\"png\", dpi=my_dpi)\n",
        "    plt.show()\n",
        "    save_list_to_txt(losses, \"img/losses_list\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPWpNVC5mC1"
      },
      "source": [
        "## Train + Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwNUT5AQnejB"
      },
      "source": [
        "import numpy\n",
        "import random\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.base import BaseEstimator\n",
        "import pickle\n",
        "\n",
        "numpy.random.seed(0)\n",
        "\n",
        "def load_partially_trained_model():\n",
        "    nlp = spacy.load(\"./saved_model\")\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "    return ner, nlp\n",
        "\n",
        "def load_spacy():\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    # Getting the pipeline component\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "    return ner, nlp\n",
        "\n",
        "\n",
        "class NerModel(BaseEstimator):\n",
        "    def __init__(self, ner, nlp, n_iter=64, dropout=0.1, lr=0.001, **model_hyper_parameters):\n",
        "        super().__init__()\n",
        "        self.ner = ner\n",
        "        self.nlp = nlp\n",
        "        self.n_iter = n_iter\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "\n",
        "    def clear_model(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "    def fit(self, train_data, eval_data):\n",
        "        \"\"\" train the Named Entity Recognition model\n",
        "\n",
        "        :param eval_data: evaluation data for testing after every epoch\n",
        "        :param train_data: processed training data\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # Adding labels to the NER\n",
        "        for _, annotations in train_data:\n",
        "            for ent in annotations.get(\"entities\"):\n",
        "                self.ner.add_label(ent[2])\n",
        "\n",
        "        # Disable pipeline components that are not changed\n",
        "        pipe_exceptions = [\"ner\"]\n",
        "        unaffected_pipes = [pipe for pipe in self.nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "        scorer = Scorer()\n",
        "\n",
        "        # Store the PRF scores for every iteration\n",
        "        train_scores = []\n",
        "        eval_scores = []\n",
        "\n",
        "        # Store losses after every iteration\n",
        "        # Each loss is itself an average of losses within a single iteration\n",
        "        loss_list = []\n",
        "\n",
        "        # Train the NER model\n",
        "        with self.nlp.select_pipes(enable=pipe_exceptions, disable=unaffected_pipes):\n",
        "            # Create a list of Examples objects\n",
        "            examples = []\n",
        "\n",
        "            for text, annots in train_data:\n",
        "                examples.append(Example.from_dict(self.nlp.make_doc(text), annots))\n",
        "\n",
        "            # Create an optimizer for the pipeline component, and set lr\n",
        "            optimizer = self.nlp.create_optimizer()\n",
        "\n",
        "            # optimizer = nlp.initialize()\n",
        "            # NOTE: Cannot use nlp.initilaize (v3) (aka nlp.begin_training for v2) on pretrained models.\n",
        "            # Use nlp.create_optimizer for training on existing model (We used pretrained en_core_web_sm).\n",
        "            # ref: https://stackoverflow.com/a/66369163/6475377\n",
        "\n",
        "            optimizer.learn_rate = self.lr\n",
        "\n",
        "            for iteration in range(ITERATIONS):\n",
        "                # print(\"Iteration: \", iteration)\n",
        "                # shuffling examples  before every iteration\n",
        "                random.shuffle(examples)\n",
        "                losses = {}\n",
        "\n",
        "                # optimizer = self.nlp.resume_training()\n",
        "\n",
        "                # batch up the examples using spaCy's minibatch\n",
        "                batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
        "                for count, batch in enumerate(batches):\n",
        "                    self.nlp.update(\n",
        "                        batch,\n",
        "                        drop=DROPOUT,  # dropout - make it harder to memorise data\n",
        "                        losses=losses,\n",
        "                        sgd=optimizer\n",
        "                    )\n",
        "\n",
        "                loss = losses[\"ner\"] / (count + 1)\n",
        "                # print(f\"Loss at epoch {iteration}: \", loss)\n",
        "                loss_list.append(loss)\n",
        "                # After training every iteration, calculate scores\n",
        "                example_list = []\n",
        "                for text, annot in train_data:\n",
        "                    # Create a Doc of our text\n",
        "                    # doc_gold_text = nlp.make_doc(text)\n",
        "                    pred_value = self.nlp(text)\n",
        "                    # reference = (Example.from_dict(doc_gold_text, annot))\n",
        "                    gold_standard = {\"entities\": annot[\"entities\"]}\n",
        "\n",
        "                    # Store prediction and gold standard ref. for each sentence\n",
        "                    # (to be used by Scorer.score)\n",
        "                    example_list.append(Example.from_dict(pred_value, gold_standard))\n",
        "\n",
        "                # Generate per-entity scores by comparing predicted with gold-standard values\n",
        "                scores = scorer.score(examples=example_list)\n",
        "                train_scores.append(scores)\n",
        "\n",
        "                # Evaluate on eval_data\n",
        "                eval_scores.append(self.evaluate(test_data=eval_data))\n",
        "                print(eval_scores[-1])\n",
        "\n",
        "        # draw_prf_graph(train_scores, keyword=\"train\")\n",
        "        # draw_prf_graph(eval_scores, keyword=\"eval\")\n",
        "        # draw_train_eval_compare_graph(train_scores, eval_scores)\n",
        "        # plot_training_loss_graph(loss_list, \"Losses with epochs\")\n",
        "\n",
        "        # Just write the last epoch's eval fscore in txt file\n",
        "        eval_fscore = []\n",
        "        for i, eval_score in enumerate(eval_scores):\n",
        "            for key, cat in eval_score.items():\n",
        "                if key == \"ents_f\": eval_fscore.append(cat)\n",
        "\n",
        "        # with open(\"k_cv_scores.txt\", 'a') as f:\n",
        "        #     f.write(\"%s\\n\" % str(eval_fscore[-1]))\n",
        "\n",
        "        # self.nlp.to_disk(\"./saved_model\")\n",
        "        return eval_fscore\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\" evaluate the trained NER model\n",
        "\n",
        "        :param test_data: processed test data\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        scorer = Scorer(self.nlp)\n",
        "        example_list = []\n",
        "\n",
        "        random.shuffle(test_data)\n",
        "\n",
        "        # Get the PRF scores for test_data\n",
        "        for text, annot in test_data:\n",
        "            # Create a Doc of our text\n",
        "            doc_gold_text = self.nlp.make_doc(text)\n",
        "\n",
        "            # Create gold-standard using the Doc of text\n",
        "            # and original (correct) entities\n",
        "            gold_standard = {\"text\": doc_gold_text, \"entities\": annot[\"entities\"]}\n",
        "\n",
        "            # Get the predictions of current test data sentence\n",
        "            pred_value = self.nlp(text)\n",
        "\n",
        "            # Create and append to the example list (of type Example) the prediction\n",
        "            # as well as the gold standard (reference)\n",
        "            example_list.append(Example.from_dict(pred_value, gold_standard))\n",
        "\n",
        "        # Generate per-entity scores by comparing predicted with gold-standard values\n",
        "        scores = scorer.score(examples=example_list)\n",
        "        return scores\n",
        "\n",
        "    def test(self, test_data):\n",
        "        \"\"\"\n",
        "        Perform final testing on unseen test_data\n",
        "        :param test_data: the unseen test data\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        print(self.evaluate(test_data))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" make inferences on unseen data\n",
        "\n",
        "        :param X: sentence to make inferences on\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.nlp = spacy.load(\"./saved_model\")\n",
        "        doc = self.nlp(X)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "   \n",
        "    def k_cross_validation(self, data, k=10):\n",
        "        print(f\"{k}-fold Cross Validation\")\n",
        "        random.shuffle(data)\n",
        "        num_groups = int(len(data) / k)\n",
        "        print(f\"Size of each eval set: {num_groups}\\n\")\n",
        "        batches = minibatch(data, size=num_groups)\n",
        "\n",
        "        for count, batch in enumerate(batches):\n",
        "            # Discard the last batch if it has very few example sentences\n",
        "            if len(batch) > num_groups / 2:\n",
        "                print(f\"Fold no.: {count + 1}\")\n",
        "                train_data = [x for x in data if x not in batch]\n",
        "                test_data = batch\n",
        "                print(f\"Train, Test :: {len(train_data)}, {len(test_data)}\")\n",
        "                fscore = self.fit(train_data=train_data, eval_data=test_data)\n",
        "                print(f\"fscore: {fscore}\\n\")\n",
        "\n",
        "            self.clear_model()\n",
        "\n",
        "\n",
        "    def train_by_parts(self, TRAIN, EVAL):\n",
        "        \"\"\"\n",
        "        Train the NER model in parts by training (1/1000)th part at a time.\n",
        "        Also, save the progress after having trained 10 such iterations. \n",
        "        \"\"\"\n",
        "        train_size_mega_iter = int(len(TRAIN) / 1000)\n",
        "        eval_size_mega_iter = int(len(EVAL) / 1000)\n",
        "\n",
        "        for mega_iteration in range(1000):\n",
        "            print(f\"Mega-iteration: {mega_iteration}\")\n",
        "            TRAIN_DATA = TRAIN[(mega_iteration * train_size_mega_iter) : ((mega_iteration + 1) * train_size_mega_iter)]\n",
        "            EVAL_DATA = EVAL[(mega_iteration * eval_size_mega_iter) : ((mega_iteration + 1) * eval_size_mega_iter)]\n",
        "            eval_fscore = self.fit(train_data=TRAIN_DATA, eval_data=EVAL_DATA)\n",
        "\n",
        "            with open(\"eval_mega_f_scores.txt\", 'a') as f:\n",
        "                for i in range(len(eval_fscore)):\n",
        "                    f.write(\"%s\\n\" % str(eval_fscore[i]))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            print(f\"Mega Eval fscore: {eval_fscore[-1]}\\n\")\n",
        "\n",
        "            # Create periodic checkpoints by saving the model after every 10th\n",
        "            # mega-iteration\n",
        "            if mega_iteration % 10 == 0:\n",
        "                self.nlp.to_disk(\"./saved_model\")\n",
        "                print(f\"Model saved @ mega-iteration: {mega_iteration}\\n\")\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs37mRXpMZCu"
      },
      "source": [
        "# Main Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2siy11fIMWuZ",
        "outputId": "c5491176-29e1-4779-a7ac-987b7aafbf94"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"spaCy version: \", spacy.__version__)\n",
        "    \n",
        "    # ner, nlp = load_spacy()\n",
        "    # DATA = load_cleaned_data()\n",
        "    # TRAIN_DATA, EVAL_DATA, TEST_DATA = split_data(DATA)\n",
        "    # save_list_to_pickle(TRAIN_DATA, \"TRAIN_DATA\")\n",
        "    # save_list_to_pickle(EVAL_DATA, \"EVAL_DATA\")\n",
        "    # save_list_to_pickle(TEST_DATA, \"TEST_DATA\")\n",
        "\n",
        "    # Load back the partially trained model from last time\n",
        "    ner, nlp = load_partially_trained_model()\n",
        "\n",
        "    # Load pickled data list from data folder\n",
        "    TRAIN_DATA = load_list_from_pickle(\"TRAIN_DATA\")\n",
        "    EVAL_DATA = load_list_from_pickle(\"EVAL_DATA\")\n",
        "    TEST_DATA = load_list_from_pickle(\"TEST_DATA\")\n",
        "\n",
        "    print(\"\\nTrain + Evaluation\")\n",
        "\n",
        "    # Create the NER model class consisting of fit and evaluate methods.\n",
        "    ner_model = NerModel(ner, nlp, n_iter=ITERATIONS, dropout=DROPOUT, lr=LEARN_RATE)\n",
        "\n",
        "    # We're gonna use TEST (5% + 5% = 10%) for evaluation\n",
        "    # TEST = EVAL_DATA + TEST_DATA\n",
        "    print(f\"Size of total TRAIN data: {len(TRAIN_DATA)}\")\n",
        "    print(f\"Size of TEST data: {len(TEST_DATA)}\")\n",
        "    print(f\"Size of EVAL data: {len(EVAL_DATA)}\\n\")\n",
        "    \n",
        "    ner_model.train_by_parts(TRAIN_DATA, EVAL_DATA)\n",
        "    \n",
        "    # ner_model.fit(TRAIN_DATA, EVAL_DATA)\n",
        "    # Perform k-fold Cross Validation\n",
        "    # ner_model.k_cross_validation(data=TRAIN_DATA + EVAL_DATA + TEST_DATA, k=10)\n",
        "\n",
        "    printf(\"\\nPerforming final testing...\")\n",
        "    ner_model.test(TEST_DATA)\n",
        "\n",
        "    # sentence = 'I really like the distortion in this guitar'\n",
        "    # ner.predict(sentence)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy version:  3.0.6\n",
            "\n",
            "Train + Evaluation\n",
            "Size of total TRAIN data: 2425500\n",
            "Size of TEST data: 134750\n",
            "Size of EVAL data: 134750\n",
            "\n",
            "Mega-iteration: 0\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9227941176470589, 'ents_r': 0.9194139194139194, 'ents_f': 0.9211009174311927, 'ents_per_type': {'INSTR': {'p': 0.937888198757764, 'r': 0.9869281045751634, 'f': 0.9617834394904458}, 'QLTY': {'p': 0.9009009009009009, 'r': 0.8333333333333334, 'f': 0.8658008658008659}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9581749049429658, 'ents_r': 0.9230769230769231, 'ents_f': 0.9402985074626866, 'ents_per_type': {'INSTR': {'p': 0.9867549668874173, 'r': 0.9738562091503268, 'f': 0.9802631578947368}, 'QLTY': {'p': 0.9196428571428571, 'r': 0.8583333333333333, 'f': 0.8879310344827586}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9628252788104089, 'ents_r': 0.9487179487179487, 'ents_f': 0.955719557195572, 'ents_per_type': {'QLTY': {'p': 0.9727272727272728, 'r': 0.8916666666666667, 'f': 0.9304347826086957}, 'INSTR': {'p': 0.9559748427672956, 'r': 0.9934640522875817, 'f': 0.9743589743589743}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9731800766283525, 'ents_r': 0.9304029304029304, 'ents_f': 0.951310861423221, 'ents_per_type': {'INSTR': {'p': 0.9743589743589743, 'r': 0.9934640522875817, 'f': 0.9838187702265372}, 'QLTY': {'p': 0.9714285714285714, 'r': 0.85, 'f': 0.9066666666666667}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9581749049429658, 'ents_r': 0.9230769230769231, 'ents_f': 0.9402985074626866, 'ents_per_type': {'QLTY': {'p': 0.9622641509433962, 'r': 0.85, 'f': 0.9026548672566371}, 'INSTR': {'p': 0.9554140127388535, 'r': 0.9803921568627451, 'f': 0.9677419354838711}}}\n",
            "Mega Eval fscore: 0.9402985074626866\n",
            "\n",
            "Model saved @ mega-iteration: 0\n",
            "\n",
            "Mega-iteration: 1\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9744680851063829, 'ents_r': 0.9662447257383966, 'ents_f': 0.9703389830508474, 'ents_per_type': {'QLTY': {'p': 0.9577464788732394, 'r': 0.9444444444444444, 'f': 0.951048951048951}, 'INSTR': {'p': 0.9817073170731707, 'r': 0.9757575757575757, 'f': 0.9787234042553191}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9703389830508474, 'ents_r': 0.9662447257383966, 'ents_f': 0.9682875264270614, 'ents_per_type': {'INSTR': {'p': 0.9640718562874252, 'r': 0.9757575757575757, 'f': 0.9698795180722892}, 'QLTY': {'p': 0.9855072463768116, 'r': 0.9444444444444444, 'f': 0.9645390070921985}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9745762711864406, 'ents_r': 0.9704641350210971, 'ents_f': 0.9725158562367865, 'ents_per_type': {'INSTR': {'p': 0.975609756097561, 'r': 0.9696969696969697, 'f': 0.9726443768996961}, 'QLTY': {'p': 0.9722222222222222, 'r': 0.9722222222222222, 'f': 0.9722222222222222}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9458333333333333, 'ents_r': 0.9578059071729957, 'ents_f': 0.9517819706498951, 'ents_per_type': {'INSTR': {'p': 0.9418604651162791, 'r': 0.9818181818181818, 'f': 0.9614243323442137}, 'QLTY': {'p': 0.9558823529411765, 'r': 0.9027777777777778, 'f': 0.9285714285714286}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9615384615384616, 'ents_r': 0.9493670886075949, 'ents_f': 0.9554140127388535, 'ents_per_type': {'INSTR': {'p': 0.9695121951219512, 'r': 0.9636363636363636, 'f': 0.9665653495440729}, 'QLTY': {'p': 0.9428571428571428, 'r': 0.9166666666666666, 'f': 0.9295774647887323}}}\n",
            "Mega Eval fscore: 0.9554140127388535\n",
            "\n",
            "Mega-iteration: 2\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9588477366255144, 'ents_r': 0.9510204081632653, 'ents_f': 0.9549180327868854, 'ents_per_type': {'QLTY': {'p': 0.9529411764705882, 'r': 0.9310344827586207, 'f': 0.941860465116279}, 'INSTR': {'p': 0.9620253164556962, 'r': 0.9620253164556962, 'f': 0.9620253164556962}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9791666666666666, 'ents_r': 0.9591836734693877, 'ents_f': 0.9690721649484536, 'ents_per_type': {'QLTY': {'p': 0.9876543209876543, 'r': 0.9195402298850575, 'f': 0.9523809523809523}, 'INSTR': {'p': 0.9748427672955975, 'r': 0.9810126582278481, 'f': 0.9779179810725552}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9349593495934959, 'ents_r': 0.9387755102040817, 'ents_f': 0.9368635437881874, 'ents_per_type': {'INSTR': {'p': 0.937888198757764, 'r': 0.9556962025316456, 'f': 0.9467084639498433}, 'QLTY': {'p': 0.9294117647058824, 'r': 0.9080459770114943, 'f': 0.9186046511627908}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9625, 'ents_r': 0.9428571428571428, 'ents_f': 0.9525773195876288, 'ents_per_type': {'INSTR': {'p': 0.967948717948718, 'r': 0.9556962025316456, 'f': 0.9617834394904459}, 'QLTY': {'p': 0.9523809523809523, 'r': 0.9195402298850575, 'f': 0.935672514619883}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9754098360655737, 'ents_r': 0.9714285714285714, 'ents_f': 0.9734151329243353, 'ents_per_type': {'INSTR': {'p': 0.9691358024691358, 'r': 0.9936708860759493, 'f': 0.9812500000000001}, 'QLTY': {'p': 0.9878048780487805, 'r': 0.9310344827586207, 'f': 0.9585798816568047}}}\n",
            "Mega Eval fscore: 0.9734151329243353\n",
            "\n",
            "Mega-iteration: 3\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9914529914529915, 'ents_r': 0.9747899159663865, 'ents_f': 0.983050847457627, 'ents_per_type': {'INSTR': {'p': 0.9938271604938271, 'r': 0.9877300613496932, 'f': 0.9907692307692307}, 'QLTY': {'p': 0.9861111111111112, 'r': 0.9466666666666667, 'f': 0.9659863945578231}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9914163090128756, 'ents_r': 0.9705882352941176, 'ents_f': 0.980891719745223, 'ents_per_type': {'QLTY': {'p': 0.9863013698630136, 'r': 0.96, 'f': 0.9729729729729729}, 'INSTR': {'p': 0.99375, 'r': 0.9754601226993865, 'f': 0.9845201238390093}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9914893617021276, 'ents_r': 0.9789915966386554, 'ents_f': 0.9852008456659619, 'ents_per_type': {'INSTR': {'p': 0.9938271604938271, 'r': 0.9877300613496932, 'f': 0.9907692307692307}, 'QLTY': {'p': 0.9863013698630136, 'r': 0.96, 'f': 0.9729729729729729}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9914529914529915, 'ents_r': 0.9747899159663865, 'ents_f': 0.983050847457627, 'ents_per_type': {'QLTY': {'p': 0.9864864864864865, 'r': 0.9733333333333334, 'f': 0.9798657718120806}, 'INSTR': {'p': 0.99375, 'r': 0.9754601226993865, 'f': 0.9845201238390093}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9914529914529915, 'ents_r': 0.9747899159663865, 'ents_f': 0.983050847457627, 'ents_per_type': {'INSTR': {'p': 0.99375, 'r': 0.9754601226993865, 'f': 0.9845201238390093}, 'QLTY': {'p': 0.9864864864864865, 'r': 0.9733333333333334, 'f': 0.9798657718120806}}}\n",
            "Mega Eval fscore: 0.983050847457627\n",
            "\n",
            "Mega-iteration: 4\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9728682170542635, 'ents_r': 0.9728682170542635, 'ents_f': 0.9728682170542635, 'ents_per_type': {'INSTR': {'p': 0.9875, 'r': 0.9753086419753086, 'f': 0.9813664596273292}, 'QLTY': {'p': 0.9489795918367347, 'r': 0.96875, 'f': 0.9587628865979382}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9768339768339769, 'ents_r': 0.9806201550387597, 'ents_f': 0.9787234042553191, 'ents_per_type': {'INSTR': {'p': 0.9817073170731707, 'r': 0.9938271604938271, 'f': 0.9877300613496932}, 'QLTY': {'p': 0.968421052631579, 'r': 0.9583333333333334, 'f': 0.9633507853403142}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9845559845559846, 'ents_r': 0.9883720930232558, 'ents_f': 0.9864603481624757, 'ents_per_type': {'INSTR': {'p': 0.9938271604938271, 'r': 0.9938271604938271, 'f': 0.9938271604938271}, 'QLTY': {'p': 0.9690721649484536, 'r': 0.9791666666666666, 'f': 0.9740932642487047}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9844961240310077, 'ents_r': 0.9844961240310077, 'ents_f': 0.9844961240310077, 'ents_per_type': {'INSTR': {'p': 0.9877300613496932, 'r': 0.9938271604938271, 'f': 0.9907692307692307}, 'QLTY': {'p': 0.9789473684210527, 'r': 0.96875, 'f': 0.9738219895287958}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.980544747081712, 'ents_r': 0.9767441860465116, 'ents_f': 0.978640776699029, 'ents_per_type': {'INSTR': {'p': 0.9817073170731707, 'r': 0.9938271604938271, 'f': 0.9877300613496932}, 'QLTY': {'p': 0.978494623655914, 'r': 0.9479166666666666, 'f': 0.962962962962963}}}\n",
            "Mega Eval fscore: 0.978640776699029\n",
            "\n",
            "Mega-iteration: 5\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9821428571428571, 'ents_r': 0.9821428571428571, 'ents_f': 0.9821428571428571, 'ents_per_type': {'INSTR': {'p': 0.9781420765027322, 'r': 0.9835164835164835, 'f': 0.9808219178082191}, 'QLTY': {'p': 0.9896907216494846, 'r': 0.9795918367346939, 'f': 0.9846153846153847}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9854545454545455, 'ents_r': 0.9678571428571429, 'ents_f': 0.9765765765765766, 'ents_per_type': {'INSTR': {'p': 0.978021978021978, 'r': 0.978021978021978, 'f': 0.978021978021978}, 'QLTY': {'p': 1.0, 'r': 0.9489795918367347, 'f': 0.9738219895287958}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.96, 'ents_r': 0.9428571428571428, 'ents_f': 0.9513513513513514, 'ents_per_type': {'INSTR': {'p': 0.9606741573033708, 'r': 0.9395604395604396, 'f': 0.95}, 'QLTY': {'p': 0.9587628865979382, 'r': 0.9489795918367347, 'f': 0.9538461538461539}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9855072463768116, 'ents_r': 0.9714285714285714, 'ents_f': 0.9784172661870504, 'ents_per_type': {'INSTR': {'p': 0.9832402234636871, 'r': 0.967032967032967, 'f': 0.9750692520775622}, 'QLTY': {'p': 0.9896907216494846, 'r': 0.9795918367346939, 'f': 0.9846153846153847}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9572953736654805, 'ents_r': 0.9607142857142857, 'ents_f': 0.9590017825311943, 'ents_per_type': {'QLTY': {'p': 0.9230769230769231, 'r': 0.9795918367346939, 'f': 0.9504950495049506}, 'INSTR': {'p': 0.9774011299435028, 'r': 0.9505494505494505, 'f': 0.9637883008356546}}}\n",
            "Mega Eval fscore: 0.9590017825311943\n",
            "\n",
            "Mega-iteration: 6\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9802631578947368, 'ents_r': 0.9770491803278688, 'ents_f': 0.9786535303776682, 'ents_per_type': {'INSTR': {'p': 0.9714285714285714, 'r': 0.9826589595375722, 'f': 0.9770114942528735}, 'QLTY': {'p': 0.9922480620155039, 'r': 0.9696969696969697, 'f': 0.9808429118773947}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9672131147540983, 'ents_r': 0.9672131147540983, 'ents_f': 0.9672131147540983, 'ents_per_type': {'INSTR': {'p': 0.9764705882352941, 'r': 0.9595375722543352, 'f': 0.9679300291545189}, 'QLTY': {'p': 0.9555555555555556, 'r': 0.9772727272727273, 'f': 0.9662921348314608}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9664429530201343, 'ents_r': 0.9442622950819672, 'ents_f': 0.9552238805970149, 'ents_per_type': {'INSTR': {'p': 0.96, 'r': 0.9710982658959537, 'f': 0.9655172413793104}, 'QLTY': {'p': 0.975609756097561, 'r': 0.9090909090909091, 'f': 0.9411764705882352}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9801980198019802, 'ents_r': 0.9737704918032787, 'ents_f': 0.9769736842105264, 'ents_per_type': {'INSTR': {'p': 0.9770114942528736, 'r': 0.9826589595375722, 'f': 0.9798270893371759}, 'QLTY': {'p': 0.9844961240310077, 'r': 0.9621212121212122, 'f': 0.9731800766283525}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9836065573770492, 'ents_r': 0.9836065573770492, 'ents_f': 0.9836065573770492, 'ents_per_type': {'QLTY': {'p': 0.9923076923076923, 'r': 0.9772727272727273, 'f': 0.9847328244274809}, 'INSTR': {'p': 0.9771428571428571, 'r': 0.9884393063583815, 'f': 0.9827586206896551}}}\n",
            "Mega Eval fscore: 0.9836065573770492\n",
            "\n",
            "Mega-iteration: 7\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9828326180257511, 'ents_r': 0.9828326180257511, 'ents_f': 0.9828326180257511, 'ents_per_type': {'QLTY': {'p': 0.9885057471264368, 'r': 0.9662921348314607, 'f': 0.9772727272727273}, 'INSTR': {'p': 0.9794520547945206, 'r': 0.9930555555555556, 'f': 0.9862068965517242}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9871244635193133, 'ents_r': 0.9871244635193133, 'ents_f': 0.9871244635193133, 'ents_per_type': {'INSTR': {'p': 0.9862068965517241, 'r': 0.9930555555555556, 'f': 0.9896193771626298}, 'QLTY': {'p': 0.9886363636363636, 'r': 0.9775280898876404, 'f': 0.983050847457627}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9785407725321889, 'ents_r': 0.9785407725321889, 'ents_f': 0.9785407725321889, 'ents_per_type': {'INSTR': {'p': 0.9793103448275862, 'r': 0.9861111111111112, 'f': 0.9826989619377162}, 'QLTY': {'p': 0.9772727272727273, 'r': 0.9662921348314607, 'f': 0.9717514124293786}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9827586206896551, 'ents_r': 0.9785407725321889, 'ents_f': 0.9806451612903226, 'ents_per_type': {'INSTR': {'p': 0.9793103448275862, 'r': 0.9861111111111112, 'f': 0.9826989619377162}, 'QLTY': {'p': 0.9885057471264368, 'r': 0.9662921348314607, 'f': 0.9772727272727273}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9828326180257511, 'ents_r': 0.9828326180257511, 'ents_f': 0.9828326180257511, 'ents_per_type': {'INSTR': {'p': 0.9929577464788732, 'r': 0.9791666666666666, 'f': 0.9860139860139859}, 'QLTY': {'p': 0.967032967032967, 'r': 0.9887640449438202, 'f': 0.9777777777777779}}}\n",
            "Mega Eval fscore: 0.9828326180257511\n",
            "\n",
            "Mega-iteration: 8\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.975103734439834, 'ents_r': 0.975103734439834, 'ents_f': 0.975103734439834, 'ents_per_type': {'QLTY': {'p': 0.9305555555555556, 'r': 0.9710144927536232, 'f': 0.9503546099290779}, 'INSTR': {'p': 0.9940828402366864, 'r': 0.9767441860465116, 'f': 0.9853372434017595}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 1.0, 'ents_f': 1.0, 'ents_per_type': {'QLTY': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.991701244813278, 'ents_r': 0.991701244813278, 'ents_f': 0.991701244813278, 'ents_per_type': {'INSTR': {'p': 0.9884393063583815, 'r': 0.9941860465116279, 'f': 0.991304347826087}, 'QLTY': {'p': 1.0, 'r': 0.9855072463768116, 'f': 0.9927007299270074}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 0.9875518672199171, 'ents_f': 0.9937369519832986, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 1.0, 'r': 0.9565217391304348, 'f': 0.9777777777777777}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9917695473251029, 'ents_r': 1.0, 'ents_f': 0.9958677685950413, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 0.971830985915493, 'r': 1.0, 'f': 0.9857142857142858}}}\n",
            "Mega Eval fscore: 0.9958677685950413\n",
            "\n",
            "Mega-iteration: 9\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 0.9959016393442623, 'ents_f': 0.997946611909651, 'ents_per_type': {'QLTY': {'p': 1.0, 'r': 0.9850746268656716, 'f': 0.9924812030075187}, 'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 1.0, 'ents_f': 1.0, 'ents_per_type': {'QLTY': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 0.9959016393442623, 'ents_f': 0.997946611909651, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 0.9943502824858758, 'f': 0.9971671388101983}, 'QLTY': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 1.0, 'ents_f': 1.0, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.991869918699187, 'ents_r': 1.0, 'ents_f': 0.9959183673469388, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 0.9710144927536232, 'r': 1.0, 'f': 0.9852941176470589}}}\n",
            "Mega Eval fscore: 0.9959183673469388\n",
            "\n",
            "Mega-iteration: 10\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.991701244813278, 'ents_r': 0.991701244813278, 'ents_f': 0.991701244813278, 'ents_per_type': {'INSTR': {'p': 0.993421052631579, 'r': 0.9869281045751634, 'f': 0.9901639344262295}, 'QLTY': {'p': 0.9887640449438202, 'r': 1.0, 'f': 0.9943502824858756}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.99581589958159, 'ents_r': 0.9875518672199171, 'ents_f': 0.9916666666666667, 'ents_per_type': {'QLTY': {'p': 1.0, 'r': 0.9886363636363636, 'f': 0.9942857142857142}, 'INSTR': {'p': 0.993421052631579, 'r': 0.9869281045751634, 'f': 0.9901639344262295}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.991701244813278, 'ents_r': 0.991701244813278, 'ents_f': 0.991701244813278, 'ents_per_type': {'QLTY': {'p': 0.9887640449438202, 'r': 1.0, 'f': 0.9943502824858756}, 'INSTR': {'p': 0.993421052631579, 'r': 0.9869281045751634, 'f': 0.9901639344262295}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 1.0, 'ents_f': 1.0, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 1.0, 'r': 1.0, 'f': 1.0}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 1.0, 'ents_r': 0.995850622406639, 'ents_f': 0.997920997920998, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'QLTY': {'p': 1.0, 'r': 0.9886363636363636, 'f': 0.9942857142857142}}}\n",
            "Mega Eval fscore: 0.997920997920998\n",
            "\n",
            "Model saved @ mega-iteration: 10\n",
            "\n",
            "Mega-iteration: 11\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9881422924901185, 'ents_r': 0.9881422924901185, 'ents_f': 0.9881422924901185, 'ents_per_type': {'INSTR': {'p': 1.0, 'r': 0.9808917197452229, 'f': 0.9903536977491961}, 'QLTY': {'p': 0.9696969696969697, 'r': 1.0, 'f': 0.9846153846153847}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9881889763779528, 'ents_r': 0.9920948616600791, 'ents_f': 0.990138067061144, 'ents_per_type': {'INSTR': {'p': 0.98125, 'r': 1.0, 'f': 0.9905362776025236}, 'QLTY': {'p': 1.0, 'r': 0.9791666666666666, 'f': 0.9894736842105264}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.9764705882352941, 'ents_r': 0.9841897233201581, 'ents_f': 0.9803149606299213, 'ents_per_type': {'INSTR': {'p': 0.9631901840490797, 'r': 1.0, 'f': 0.98125}, 'QLTY': {'p': 1.0, 'r': 0.9583333333333334, 'f': 0.9787234042553191}}}\n",
            "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.996031746031746, 'ents_r': 0.9920948616600791, 'ents_f': 0.994059405940594, 'ents_per_type': {'QLTY': {'p': 1.0, 'r': 0.9791666666666666, 'f': 0.9894736842105264}, 'INSTR': {'p': 0.9936708860759493, 'r': 1.0, 'f': 0.9968253968253968}}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-314c682146eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Size of EVAL data: {len(EVAL_DATA)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_by_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# ner_model.fit(TRAIN_DATA, EVAL_DATA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Perform k-fold Cross Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-7577dfb4f6a5>\u001b[0m in \u001b[0;36mtrain_by_parts\u001b[0;34m(self, TRAIN, EVAL)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mTRAIN_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_iteration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_size_mega_iter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_size_mega_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mEVAL_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEVAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_iteration\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meval_size_mega_iter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmega_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meval_size_mega_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0meval_fscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVAL_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_mega_f_scores.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-7577dfb4f6a5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, eval_data)\u001b[0m\n\u001b[1;32m     91\u001b[0m                         \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                         \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                         \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                     )\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/ml/parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.step_forward.backprop_parser_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/thinc/layers/linear.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(dY)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36minc_grad\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minc_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloatsXd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;34m\"\"\"Increment the gradient of a parameter by a value.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/thinc/backends/_param_server.py\u001b[0m in \u001b[0;36minc_grad\u001b[0;34m(self, model_id, name, value)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d88ebd2-24af-4fc2-cf43-09bda5afb2d9"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "!zip -r /content/data.zip /content/data\n",
        "# !zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/TEST_DATA.pkl (deflated 66%)\n",
            "  adding: content/data/TRAIN_DATA.pkl (deflated 66%)\n",
            "  adding: content/data/EVAL_DATA.pkl (deflated 66%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}