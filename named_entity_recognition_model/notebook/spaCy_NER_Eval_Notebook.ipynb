{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of enormous dataset size\n",
        "\n",
        "* Link to the dataset: https://drive.google.com/file/d/1qdjtMgGyafPNN_x2EANkcGLz5S-SB8mG/view?usp=sharing\n",
        "\n",
        "* The dataset is of size 2.6 GB, and contains more than 2M sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gwOh7y7NQ6"
      },
      "source": [
        "## Mount Google Drive to get acccess to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQdvZLkhFHs7",
        "outputId": "81df4ed1-05eb-4aa1-ee7f-dfa7f595267b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0JtYDT_Dx7S",
        "outputId": "29c2d84f-639c-4879-b5b2-2b852ab462ea"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 21 21:55:34 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    75W / 149W |   7601MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBp28TAJmhB"
      },
      "source": [
        "## Install torch\n",
        "\n",
        "* Install torch specifc to the Google Colab's CUDA version\n",
        "* CUDA version 11.1 works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LAGZmbvE0VQ"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud_az1oZnELb"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7I12l76nCEv"
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "ITERATIONS = 5\n",
        "DROPOUT = 0.1\n",
        "LEARN_RATE = 0.001\n",
        "\n",
        "DATA_PATH = \"./data_filter_csv.csv\"\n",
        "\n",
        "# Path to the 2.6 GB augmented data\n",
        "BIG_DATA_PATH = \"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset.csv\"\n",
        "\n",
        "\n",
        "def load_cleaned_data(data_path=DATA_PATH):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    col_names = ['text', 'entities']\n",
        "\n",
        "    data = pd.read_csv(data_path, names=col_names)\n",
        "    # print(data.head())\n",
        "    entity_list = data.entities.to_list()\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"split_sentences\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        annot_list = []\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # print(index)\n",
        "        # print(data['text'][index].lower())\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "\n",
        "                # Remove any leading or beginning blank space\n",
        "                word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "                start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "                start_index = start_index + 0\n",
        "                end_index = start_index + len(word_pair_list[0])\n",
        "\n",
        "                # Incase word not found in the sentence\n",
        "                if start_index == -1:\n",
        "                    print(\"-1 error\")\n",
        "                    print(data['text'][index])\n",
        "                    break\n",
        "\n",
        "                both_present = lambda: (start_index, end_index) in indices_list\n",
        "                start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "                end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "                left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "                def right_blank():\n",
        "                    # return true if there is no blank space after the end_index,\n",
        "                    # as long as end_index is not at the end of the sentence\n",
        "                    if len(data['text'][index].lower()) != end_index:\n",
        "                        return data['text'][index][end_index] != \" \"\n",
        "                \n",
        "                # Check if this start_index and/or end_index is already in the list:\n",
        "                # (To prevent overlapping with already tagged words)\n",
        "                flag = 0\n",
        "                while True:\n",
        "                    if (start_index == -1 or end_index == -1):\n",
        "                        flag = 1\n",
        "                        break\n",
        "                    if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                    \n",
        "                        start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                           start=end_index + 1).astype(\n",
        "                            numpy.int64)\n",
        "                        start_index = start_index + 0\n",
        "                        end_index = start_index + len(word_pair_list[0])\n",
        "\n",
        "                    else:\n",
        "                        indices_list.append((start_index, end_index))\n",
        "                        break\n",
        "                \n",
        "                if (flag == 1):\n",
        "                    # Don't bother checking rest of the current sentence\n",
        "                    break\n",
        "                \n",
        "                annot_list.append((start_index, end_index, word_pair_list[1]))\n",
        "        # print(data['text'][index].lower())\n",
        "        # print(annot_list)\n",
        "        DATA.append((data['text'][index].lower(), {\"entities\": annot_list}))\n",
        "\n",
        "    # save_list_to_txt(DATA)\n",
        "    return DATA\n",
        "\n",
        "\n",
        "def save_list_to_txt(data, keyword):\n",
        "    with open(keyword + \".txt\", 'w') as f:\n",
        "        for item in data:\n",
        "            f.write(\"%s\\n\" % str(item))\n",
        "\n",
        "\n",
        "def save_list_to_pickle(list, name):\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    with open('data/' + name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(list, f)\n",
        "\n",
        "\n",
        "def load_list_from_pickle(filename):\n",
        "    with open(\"data/\" + filename + '.pkl', 'rb') as f:\n",
        "        list = pickle.load(f)\n",
        "    return list\n",
        "\n",
        "\n",
        "def split_data(DATA):\n",
        "    random.shuffle(DATA)\n",
        "\n",
        "    # Randomly pull out 10 % segments of DATA for test + eval\n",
        "    test_length = math.floor((10 / 100) * len(DATA))\n",
        "    TEST = DATA[:test_length]\n",
        "\n",
        "    random.shuffle(TEST)\n",
        "    # Randomly pull out 50 % segments of TEST_DATA for EVAL_DATA\n",
        "    eval_length = math.floor((50 / 100) * len(TEST))\n",
        "    EVAL_DATA = TEST[:eval_length]\n",
        "    TEST_DATA = TEST[eval_length:len(TEST)]\n",
        "\n",
        "    # Time consuming for our BIG data\n",
        "    # TRAIN_DATA = [x for x in DATA if x not in TEST]\n",
        "\n",
        "    TRAIN_DATA = DATA[test_length:len(DATA)]\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"\\nTotal sentences: \", len(DATA))\n",
        "    print(\"Length of train data: \", len(TRAIN_DATA))\n",
        "    print(\"Length of evaluation data: \", len(EVAL_DATA))\n",
        "    print(\"Length of test data: \", len(TEST_DATA))\n",
        "\n",
        "    TEST.clear()\n",
        "\n",
        "    return TRAIN_DATA, EVAL_DATA, TEST_DATA\n",
        "\n",
        "\n",
        "def plot_graph(title, keyword, precision=None, recall=None, fscore=None):\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "    legend_to_show = ()\n",
        "    if precision is not None:\n",
        "        plt.plot(x, precision, color='red', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='red', markersize=2)\n",
        "        legend_to_show += (\"precision\",)\n",
        "    if recall is not None:\n",
        "        plt.plot(x, recall, color='blue', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='blue', markersize=2)\n",
        "        legend_to_show += (\"recall\",)\n",
        "    if fscore is not None:\n",
        "        plt.plot(x, fscore, color='green', linestyle='solid', linewidth=1,\n",
        "                 marker='o', markerfacecolor='green', markersize=2)\n",
        "        legend_to_show += (\"fscore\",)\n",
        "    plt.gca().legend(legend_to_show, loc='best')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "\n",
        "    plt.title(title + \" PRF Scores [\" + keyword + \"]\")\n",
        "\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"img\"):\n",
        "        os.makedirs(\"img\")\n",
        "\n",
        "    plt.savefig(\"img/plot_\" + title + \"_\" + keyword + \".png\", format=\"png\", dpi=my_dpi)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "def draw_prf_graph(train_scores, keyword=\"\", overall=True, instr=True, qlty=True, edge=True):\n",
        "    precision = []\n",
        "    recall = []\n",
        "    fscore = []\n",
        "\n",
        "    qlty_p = []\n",
        "    qlty_r = []\n",
        "    qlty_f = []\n",
        "\n",
        "    instr_p = []\n",
        "    instr_r = []\n",
        "    instr_f = []\n",
        "\n",
        "    edge_p = []\n",
        "    edge_r = []\n",
        "    edge_f = []\n",
        "\n",
        "    # Extract P, R, F from train_score\n",
        "    for i, train_score in enumerate(train_scores):\n",
        "        for key, cat in train_score.items():\n",
        "            if key == \"ents_p\": precision.append(cat)\n",
        "            if key == \"ents_r\": recall.append(cat)\n",
        "            if key == \"ents_f\": fscore.append(cat)\n",
        "            if key == \"ents_per_type\":\n",
        "                for attribute, value in cat.items():\n",
        "                    if attribute == \"QLTY\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": qlty_p.append(sc)\n",
        "                            if k == \"r\": qlty_r.append(sc)\n",
        "                            if k == \"f\": qlty_f.append(sc)\n",
        "                    if attribute == \"INSTR\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": instr_p.append(sc)\n",
        "                            if k == \"r\": instr_r.append(sc)\n",
        "                            if k == \"f\": instr_f.append(sc)\n",
        "                    if attribute == \"EDGE\":\n",
        "                        for k, sc in value.items():\n",
        "                            if k == \"p\": edge_p.append(sc)\n",
        "                            if k == \"r\": edge_r.append(sc)\n",
        "                            if k == \"f\": edge_f.append(sc)\n",
        "\n",
        "    if overall is True:\n",
        "        plot_graph(title=keyword, keyword=\"overall\", precision=precision, recall=recall,\n",
        "                   fscore=fscore)\n",
        "    if qlty is True:\n",
        "        plot_graph(title=keyword, keyword=\"qlty\", precision=qlty_p, recall=qlty_r, fscore=qlty_f)\n",
        "    if instr is True:\n",
        "        plot_graph(title=keyword, keyword=\"instr\", precision=instr_p, recall=instr_r,\n",
        "                   fscore=instr_f)\n",
        "    if edge is True:\n",
        "        plot_graph(title=keyword, keyword=\"edge\", precision=edge_p, recall=edge_r, fscore=edge_f)\n",
        "\n",
        "\n",
        "def draw_train_eval_compare_graph(train_scores, eval_scores):\n",
        "    train_fscore = []\n",
        "    eval_fscore = []\n",
        "\n",
        "    for i, train_score in enumerate(train_scores):\n",
        "        for key, cat in train_score.items():\n",
        "            if key == \"ents_f\": train_fscore.append(cat)\n",
        "\n",
        "    for i, eval_score in enumerate(eval_scores):\n",
        "        for key, cat in eval_score.items():\n",
        "            if key == \"ents_f\": eval_fscore.append(cat)\n",
        "\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "\n",
        "    poly_order = 4\n",
        "\n",
        "    plt.plot(x, train_fscore, color='red', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='red', markersize=2)\n",
        "    train_reg_line = np.polyfit(np.array(x), np.array(train_fscore), poly_order)\n",
        "    p = np.poly1d(train_reg_line)\n",
        "    plt.plot(x, p(x), color='red', linestyle='--', linewidth=0.6,\n",
        "             marker='o', markerfacecolor='red', markersize=1, label='_nolegend_')\n",
        "\n",
        "    plt.plot(x, eval_fscore, color='blue', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='blue', markersize=2)\n",
        "    eval_reg_line = np.polyfit(np.array(x), np.array(eval_fscore), poly_order)\n",
        "    p = np.poly1d(eval_reg_line)\n",
        "    plt.plot(x, p(x), color='blue', linestyle='--', linewidth=0.6,\n",
        "             marker='o', markerfacecolor='blue', markersize=1, label='_nolegend_')\n",
        "\n",
        "    plt.gca().legend((\"train\", \"eval\"), loc='best')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(\"F-Score vs Epochs\")\n",
        "    plt.ylim(0.00, 1.00)\n",
        "\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"img\"):\n",
        "        os.makedirs(\"img\")\n",
        "        \n",
        "    plt.savefig(\"img/plot_fscore_train_vs_eval.png\", format=\"png\", dpi=my_dpi)\n",
        "    plt.savefig(\"img/plot_fscore_train_vs_eval.svg\", format=\"svg\", dpi=my_dpi)\n",
        "    plt.savefig(\"img/plot_fscore_train_vs_eval.pdf\", format=\"pdf\", dpi=my_dpi)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_training_loss_graph(losses, title):\n",
        "    my_dpi = 200\n",
        "    plt.rcParams['figure.figsize'] = 10, 5\n",
        "    plt.figure(figsize=(1280 / my_dpi, 720 / my_dpi), dpi=my_dpi)\n",
        "    x = list(range(1, ITERATIONS + 1))\n",
        "    plt.plot(x, losses, color='blue', linestyle='solid', linewidth=1,\n",
        "             marker='o', markerfacecolor='green', markersize=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "\n",
        "    # If the directory does not exist, create it\n",
        "    if not os.path.exists(\"img\"):\n",
        "        os.makedirs(\"img\")\n",
        "\n",
        "    plt.savefig(\"img/plot_loss_training\" + \".png\", format=\"png\", dpi=my_dpi)\n",
        "    plt.savefig(\"img/plot_loss_training\" + \".svg\", format=\"svg\", dpi=my_dpi)\n",
        "    plt.savefig(\"img/plot_loss_training\" + \".pdf\", format=\"pdf\", dpi=my_dpi)\n",
        "    plt.show()\n",
        "    save_list_to_txt(losses, \"img/losses_list\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPWpNVC5mC1"
      },
      "source": [
        "## Train + Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwNUT5AQnejB"
      },
      "source": [
        "import numpy\n",
        "import random\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training import Example\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.base import BaseEstimator\n",
        "import pickle\n",
        "\n",
        "numpy.random.seed(0)\n",
        "\n",
        "def load_partially_trained_model():\n",
        "    nlp = spacy.load(\"./saved_model\")\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "    return ner, nlp\n",
        "\n",
        "def load_spacy():\n",
        "    spacy.require_gpu()\n",
        "\n",
        "    # 1) 'Makeshift' method\n",
        "    # TODO: Verify this methodology\n",
        "    source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "    source_nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    nlp.add_pipe(\"transformer\", source=source_nlp_trf)\n",
        "    nlp.add_pipe(\"ner\", source=source_nlp)\n",
        "    \n",
        "    # 2) trf only method (does NOT work)\n",
        "    # nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "    # Getting the pipeline component\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "    return ner, nlp\n",
        "\n",
        "\n",
        "class NerModel(BaseEstimator):\n",
        "    def __init__(self, ner, nlp, n_iter=64, dropout=0.1, lr=0.001, **model_hyper_parameters):\n",
        "        super().__init__()\n",
        "        self.ner = ner\n",
        "        self.nlp = nlp\n",
        "        self.n_iter = n_iter\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "\n",
        "    def clear_model(self):\n",
        "        # self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        # self.ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "        source_nlp = spacy.load(\"en_core_web_sm\")\n",
        "        source_nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "        self.nlp = spacy.blank(\"en\")\n",
        "        self.nlp.add_pipe(\"transformer\", source=source_nlp_trf)\n",
        "        self.nlp.add_pipe(\"ner\", source=source_nlp)\n",
        "        \n",
        "        self.ner = self.nlp.get_pipe(\"ner\")\n",
        "\n",
        "    def fit(self, train_data, eval_data):\n",
        "        \"\"\" train the Named Entity Recognition model\n",
        "\n",
        "        :param eval_data: evaluation data for testing after every epoch\n",
        "        :param train_data: processed training data\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # Adding labels to the NER\n",
        "        for _, annotations in train_data:\n",
        "            for ent in annotations.get(\"entities\"):\n",
        "                self.ner.add_label(ent[2])\n",
        "\n",
        "        # Disable pipeline components that are not changed\n",
        "        # print(self.nlp.pipe_names)\n",
        "        pipe_exceptions = [\"ner\", \"transformer\"]\n",
        "        unaffected_pipes = [pipe for pipe in self.nlp.pipe_names if pipe not in pipe_exceptions]\n",
        "\n",
        "        scorer = Scorer()\n",
        "\n",
        "        # Store the PRF scores for every iteration\n",
        "        train_scores = []\n",
        "        eval_scores = []\n",
        "\n",
        "        # Store losses after every iteration\n",
        "        # Each loss is itself an average of losses within a single iteration\n",
        "        loss_list = []\n",
        "\n",
        "        # Train the NER model\n",
        "        with self.nlp.select_pipes(enable=pipe_exceptions, disable=unaffected_pipes):\n",
        "            # Create a list of Examples objects\n",
        "            examples = []\n",
        "\n",
        "            for text, annots in train_data:\n",
        "                examples.append(Example.from_dict(self.nlp.make_doc(text), annots))\n",
        "\n",
        "            # Create an optimizer for the pipeline component, and set lr\n",
        "            optimizer = self.nlp.create_optimizer()\n",
        "\n",
        "            # optimizer = nlp.initialize()\n",
        "            # NOTE: Cannot use nlp.initilaize (v3) (aka nlp.begin_training for v2) on pretrained models.\n",
        "            # Use nlp.create_optimizer for training on existing model (We used pretrained en_core_web_sm).\n",
        "            # ref: https://stackoverflow.com/a/66369163/6475377\n",
        "\n",
        "            optimizer.learn_rate = self.lr\n",
        "\n",
        "            for iteration in range(ITERATIONS):\n",
        "                # print(\"Iteration: \", iteration)\n",
        "                # shuffling examples  before every iteration\n",
        "                random.shuffle(examples)\n",
        "                losses = {}\n",
        "\n",
        "                # optimizer = self.nlp.resume_training()\n",
        "\n",
        "                # batch up the examples using spaCy's minibatch\n",
        "                batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
        "                for count, batch in enumerate(batches):\n",
        "                    self.nlp.update(\n",
        "                        batch,\n",
        "                        drop=DROPOUT,  # dropout - make it harder to memorise data\n",
        "                        losses=losses,\n",
        "                        sgd=optimizer\n",
        "                    )\n",
        "\n",
        "                loss = losses[\"ner\"] / (count + 1)\n",
        "                # print(f\"Loss at epoch {iteration}: \", loss)\n",
        "                loss_list.append(loss)\n",
        "                # After training every iteration, calculate scores\n",
        "                example_list = []\n",
        "                for text, annot in train_data:\n",
        "                    # Create a Doc of our text\n",
        "                    # doc_gold_text = nlp.make_doc(text)\n",
        "                    pred_value = self.nlp(text)\n",
        "                    # reference = (Example.from_dict(doc_gold_text, annot))\n",
        "                    gold_standard = {\"entities\": annot[\"entities\"]}\n",
        "\n",
        "                    # Store prediction and gold standard ref. for each sentence\n",
        "                    # (to be used by Scorer.score)\n",
        "                    example_list.append(Example.from_dict(pred_value, gold_standard))\n",
        "\n",
        "                # Generate per-entity scores by comparing predicted with gold-standard values\n",
        "                scores = scorer.score(examples=example_list)\n",
        "                train_scores.append(scores)\n",
        "\n",
        "                # Evaluate on eval_data\n",
        "                eval_scores.append(self.evaluate(test_data=eval_data))\n",
        "                # print(eval_scores[-1])\n",
        "\n",
        "        # draw_prf_graph(train_scores, keyword=\"train\")\n",
        "        # draw_prf_graph(eval_scores, keyword=\"eval\")\n",
        "        draw_train_eval_compare_graph(train_scores, eval_scores)\n",
        "        plot_training_loss_graph(loss_list, \"Losses with epochs\")\n",
        "\n",
        "        # Just write the last epoch's eval fscore in txt file\n",
        "        eval_fscore = []\n",
        "        for i, eval_score in enumerate(eval_scores):\n",
        "            for key, cat in eval_score.items():\n",
        "                if key == \"ents_f\": eval_fscore.append(cat)\n",
        "\n",
        "        with open(\"k_cv_scores.txt\", 'a') as f:\n",
        "            f.write(\"%s\\n\" % str(eval_fscore[-1]))\n",
        "\n",
        "        self.nlp.to_disk(\"./saved_model\")\n",
        "        return eval_fscore\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\" evaluate the trained NER model\n",
        "\n",
        "        :param test_data: processed test data\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        scorer = Scorer(self.nlp)\n",
        "        example_list = []\n",
        "\n",
        "        random.shuffle(test_data)\n",
        "\n",
        "        # Get the PRF scores for test_data\n",
        "        for text, annot in test_data:\n",
        "            # Create a Doc of our text\n",
        "            doc_gold_text = self.nlp.make_doc(text)\n",
        "\n",
        "            # Create gold-standard using the Doc of text\n",
        "            # and original (correct) entities\n",
        "            gold_standard = {\"text\": doc_gold_text, \"entities\": annot[\"entities\"]}\n",
        "\n",
        "            # Get the predictions of current test data sentence\n",
        "            pred_value = self.nlp(text)\n",
        "\n",
        "            # Create and append to the example list (of type Example) the prediction\n",
        "            # as well as the gold standard (reference)\n",
        "            example_list.append(Example.from_dict(pred_value, gold_standard))\n",
        "\n",
        "        # Generate per-entity scores by comparing predicted with gold-standard values\n",
        "        scores = scorer.score(examples=example_list)\n",
        "        return scores\n",
        "\n",
        "    def test(self, test_data):\n",
        "        \"\"\"\n",
        "        Perform final testing on unseen test_data\n",
        "        :param test_data: the unseen test data\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        print(self.evaluate(test_data))\n",
        "        print(\"Done.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" make inferences on unseen data\n",
        "\n",
        "        :param X: sentence to make inferences on\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.nlp = spacy.load(\"./saved_model\")\n",
        "        doc = self.nlp(X)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "   \n",
        "    def k_cross_validation(self, data, k=10):\n",
        "        print(f\"{k}-fold Cross Validation\")\n",
        "        random.shuffle(data)\n",
        "        num_groups = int(len(data) / k)\n",
        "        print(f\"Size of each eval set: {num_groups}\\n\")\n",
        "        batches = minibatch(data, size=num_groups)\n",
        "\n",
        "        for count, batch in enumerate(batches):\n",
        "            # Discard the last batch if it has very few example sentences\n",
        "            if len(batch) > num_groups / 2:\n",
        "                print(f\"Fold no.: {count + 1}\")\n",
        "                train_data = [x for x in data if x not in batch]\n",
        "                test_data = batch\n",
        "                print(f\"Train, Test :: {len(train_data)}, {len(test_data)}\")\n",
        "                fscore = self.fit(train_data=train_data, eval_data=test_data)\n",
        "                print(f\"fscore: {fscore}\\n\")\n",
        "\n",
        "            self.clear_model()\n",
        "\n",
        "\n",
        "    def train_by_parts(self, TRAIN, EVAL):\n",
        "        \"\"\"\n",
        "        Train the NER model in parts by training (1/1000)th part at a time.\n",
        "        Also, save the progress after having trained 10 such iterations. \n",
        "        \"\"\"\n",
        "        train_size_mega_iter = int(len(TRAIN) / 1000)\n",
        "        eval_size_mega_iter = int(len(EVAL) / 1000)\n",
        "\n",
        "        for mega_iteration in range(1000):\n",
        "            print(f\"Mega-iteration: {mega_iteration}\")\n",
        "            TRAIN_DATA = TRAIN[(mega_iteration * train_size_mega_iter) : ((mega_iteration + 1) * train_size_mega_iter)]\n",
        "            EVAL_DATA = EVAL[(mega_iteration * eval_size_mega_iter) : ((mega_iteration + 1) * eval_size_mega_iter)]\n",
        "            eval_fscore = self.fit(train_data=TRAIN_DATA, eval_data=EVAL_DATA)\n",
        "\n",
        "            with open(\"eval_mega_f_scores.txt\", 'a') as f:\n",
        "                for i in range(len(eval_fscore)):\n",
        "                    f.write(\"%s\\n\" % str(eval_fscore[i]))\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            print(f\"Mega Eval fscore: {eval_fscore[-1]}\\n\")\n",
        "\n",
        "            # Create periodic checkpoints by saving the model after every 10th\n",
        "            # mega-iteration\n",
        "            if mega_iteration % 10 == 0:\n",
        "                self.nlp.to_disk(\"./saved_model\")\n",
        "                print(f\"Model saved @ mega-iteration: {mega_iteration}\\n\")\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs37mRXpMZCu"
      },
      "source": [
        "# Main Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2siy11fIMWuZ"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"spaCy version: \", spacy.__version__)\n",
        "    \n",
        "    ner, nlp = load_spacy()\n",
        "    # DATA = load_cleaned_data()\n",
        "    # TRAIN_DATA, EVAL_DATA, TEST_DATA = split_data(DATA)\n",
        "    # save_list_to_pickle(TRAIN_DATA, \"TRAIN_DATA\")\n",
        "    # save_list_to_pickle(EVAL_DATA, \"EVAL_DATA\")\n",
        "    # save_list_to_pickle(TEST_DATA, \"TEST_DATA\")\n",
        "\n",
        "    # Load back the partially trained model from last time\n",
        "    # ner, nlp = load_partially_trained_model()\n",
        "\n",
        "    # Load pickled data list from data folder\n",
        "    TRAIN_DATA = load_list_from_pickle(\"TRAIN_DATA\")\n",
        "    EVAL_DATA = load_list_from_pickle(\"EVAL_DATA\")\n",
        "    TEST_DATA = load_list_from_pickle(\"TEST_DATA\")\n",
        "\n",
        "    print(\"\\nTrain + Evaluation\")\n",
        "\n",
        "    # Create the NER model class consisting of fit and evaluate methods.\n",
        "    ner_model = NerModel(ner, nlp, n_iter=ITERATIONS, dropout=DROPOUT, lr=LEARN_RATE)\n",
        "\n",
        "    # We're gonna use TEST (5% + 5% = 10%) for evaluation\n",
        "    # TEST = EVAL_DATA + TEST_DATA\n",
        "    print(f\"Size of total TRAIN data: {len(TRAIN_DATA)}\")\n",
        "    print(f\"Size of TEST data: {len(TEST_DATA)}\")\n",
        "    print(f\"Size of EVAL data: {len(EVAL_DATA)}\\n\")\n",
        "    \n",
        "    # ner_model.train_by_parts(TRAIN_DATA, EVAL_DATA)\n",
        "    \n",
        "    ner_model.fit(TRAIN_DATA, EVAL_DATA)\n",
        "    # Perform k-fold Cross Validation\n",
        "    # ner_model.k_cross_validation(data=TRAIN_DATA + EVAL_DATA + TEST_DATA, k=10)\n",
        "\n",
        "    print(\"\\nPerforming final testing...\")\n",
        "    ner_model.test(TEST_DATA)\n",
        "\n",
        "    # sentence = 'I really like the distortion in this guitar'\n",
        "    # ner.predict(sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "# !unzip /content/saved_model.zip\n",
        "# !zip -r /content/data.zip /content/data\n",
        "!zip -r /content/img.zip /content/img\n",
        "# !zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}