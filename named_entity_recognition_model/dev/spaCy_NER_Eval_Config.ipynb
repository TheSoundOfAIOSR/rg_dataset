{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "spaCy_NER_Eval_Config.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "dYR2u9U0_JLB",
    "-qCBw9tx5f9R"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAQUUODx7h4v"
   },
   "source": [
    "# NER Evaluation of Augmented data\n",
    "\n",
    "* This evaluation is done in Google Colab because of:\n",
    "    * Enormous dataset size\n",
    "    * Transformer based architecture involving GPU usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYR2u9U0_JLB"
   },
   "source": [
    "## 1. Install spaCy and download English model file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zRBO4JulnpP7"
   },
   "source": [
    "# !pip install cupy-cuda112\n",
    "!pip install spacy==3.0.6"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UaJC5o7XpHOs"
   },
   "source": [
    "# Download spacy small model\n",
    "# !python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l0JtYDT_Dx7S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7acb11a4-47ca-4f59-c1f0-6ff5919d1fa9"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Sun Jul  4 14:16:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJBp28TAJmhB"
   },
   "source": [
    "## 2. Install torch\n",
    "\n",
    "* Install torch specifc to the Google Colab's CUDA version\n",
    "* CUDA version 11.1 works\n",
    "* Update: Not really required (on Google Colab atleast), `en_core_web_trf` suffices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LAGZmbvE0VQ"
   },
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZDk28NRC_s8"
   },
   "source": [
    "## 3. Pre-process and save to jsonl\n",
    "\n",
    "* Section 3.3 and 3.4 process the csv files all at once\n",
    "* Section 3.5 processes the CSV files in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyNJ16vJ5Xov"
   },
   "source": [
    "### 3.1 Extract the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wph-DdQFKPnO"
   },
   "source": [
    "!unzip /content/augmented_dataset_2021-06-30.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qCBw9tx5f9R"
   },
   "source": [
    "### 3.2 Loader function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FDjtHcwhDMYD"
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy\n",
    "from numpy.core.defchararray import find\n",
    "\n",
    "TRAIN_DATA_PATH = \"./augmented_dataset_2021-06-30/train.csv\"\n",
    "TEST_CONTENT_DATA_PATH = \"./augmented_dataset_2021-06-30/test_content.csv\"\n",
    "TEST_CONTEXT_DATA_PATH = \"./augmented_dataset_2021-06-30/test_context.csv\"\n",
    "TEST_UNSEEN = \"./augmented_dataset_2021-06-30/test_unseen.csv\"\n",
    "\n",
    "def load_cleaned_data(data_path, train_data_only=None, train_data_pd=None):\n",
    "    \"\"\"\n",
    "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
    "    and calculate the start and end index.\n",
    "    After getting the (start, end) pair, check if this pair was already calculated\n",
    "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
    "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
    "    :return: DATA\n",
    "    \"\"\"\n",
    "    if train_data_only is None:\n",
    "        col_names = ['text', 'entities']\n",
    "\n",
    "        data = pd.read_csv(data_path, names=col_names, usecols=[0, 1])\n",
    "        entity_list = data.entities.to_list()\n",
    "\n",
    "    else:\n",
    "        # Incoming `train_data_only` is itself a pandas,\n",
    "        # so just process it.\n",
    "        entity_list = train_data_only\n",
    "        data = train_data_pd\n",
    "\n",
    "    DATA = []\n",
    "\n",
    "    for index, ent in enumerate(entity_list):\n",
    "        if ent == \"tokens\":\n",
    "            continue\n",
    "\n",
    "        ent = ent.split(\"), (\")\n",
    "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
    "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
    "\n",
    "        # Initialize index list, to store pairs of (start, end) indices\n",
    "        indices_list = [(-1, -1), (-1, -1)]\n",
    "\n",
    "        tokens_list = []\n",
    "        spans_list = []\n",
    "\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "\n",
    "        # Analyze current \"split_sentences\"'s all word-pairs\n",
    "        for index_ent, word_pair in enumerate(ent):\n",
    "            word_pair_list = []\n",
    "            \n",
    "            # Split the word and its pair\n",
    "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
    "\n",
    "            # Remove any leading or beginning blank space\n",
    "            word_pair_list[0] = word_pair_list[0].strip()\n",
    "\n",
    "            start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
    "            start_index = int(start_index + 0)\n",
    "            end_index = int(start_index + len(word_pair_list[0]))\n",
    "\n",
    "            # Incase word not found in the sentence\n",
    "            if start_index == -1:\n",
    "                print(\"\\n-1 error\")\n",
    "                print(\"Couldn't find:\")\n",
    "                print(word_pair_list[0])\n",
    "                print(\"in:\")\n",
    "                print(data['text'][index])\n",
    "                break\n",
    "\n",
    "            both_present = lambda: (start_index, end_index) in indices_list\n",
    "            start_present = lambda: start_index in [i[0] for i in indices_list]\n",
    "            end_present = lambda: end_index in [i[1] for i in indices_list]\n",
    "            left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
    "\n",
    "            def right_blank():\n",
    "                # return true if there is no blank space after the end_index,\n",
    "                # as long as end_index is not at the end of the sentence\n",
    "                if len(data['text'][index].lower()) != end_index:\n",
    "                    return data['text'][index][end_index] != \" \"\n",
    "            \n",
    "            # Check if this start_index and/or end_index is already in the list:\n",
    "            # (To prevent overlapping with already tagged words)\n",
    "            flag = 0\n",
    "            while True:\n",
    "                if (start_index == -1 or end_index == -1):\n",
    "                    flag = 1\n",
    "                    break\n",
    "                if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
    "                \n",
    "                    start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
    "                                        start=end_index + 1).astype(numpy.int64)\n",
    "                    start_index = int(start_index + 0)\n",
    "                    end_index = int(start_index + len(word_pair_list[0]))\n",
    "\n",
    "                else:\n",
    "                    indices_list.append((start_index, end_index))\n",
    "                    break\n",
    "            \n",
    "            if (flag == 1):\n",
    "                # Don't bother checking rest of the current sentence\n",
    "                break\n",
    "            \n",
    "            # Add ALL the words and their positions to a \"tokens\" list\n",
    "            tokens_list.append({\"text\": word_pair_list[0], \"start\": start_index, \"end\": end_index})\n",
    "\n",
    "            # Add the specially tagged words to a \"spans\" list\n",
    "            if word_pair_list[1] != \"NONE\":\n",
    "                spans_list.append({\"start\": start_index, \"end\": end_index, \"label\": word_pair_list[1]})\n",
    "\n",
    "        DATA.append({\"text\": data['text'][index].lower(), \"tokens\": tokens_list, \"spans\": spans_list, \"answer\": \"accept\"})\n",
    "        \n",
    "    return DATA\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTRXgNMfy26a"
   },
   "source": [
    "### 3.3 Convert the CSV files to Python list"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XEp1754vyr0k"
   },
   "source": [
    "# TRAIN_DATA = load_cleaned_data(TRAIN_DATA_PATH)\n",
    "# TEST_CONTENT = load_cleaned_data(TEST_CONTENT_DATA_PATH)\n",
    "# TEST_CONTEXT = load_cleaned_data(TEST_CONTEXT_DATA_PATH)\n",
    "UNSEEN_DATA = load_cleaned_data(TEST_UNSEEN)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaFqxUFg5jTv"
   },
   "source": [
    "### 3.4 Save to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0QamFbSfcXJT"
   },
   "source": [
    "import json\n",
    "\n",
    "if not os.path.exists(\"assets\"):\n",
    "        os.makedirs(\"assets\")\n",
    "\n",
    "# with open('assets/TRAIN_DATA.jsonl', 'w') as f:\n",
    "#     for entry in TRAIN_DATA:\n",
    "#         json.dump(entry, f)\n",
    "#         f.write('\\n')\n",
    "\n",
    "# with open('assets/TEST_CONTENT.jsonl', 'w') as f:\n",
    "#     for entry in TEST_CONTENT:\n",
    "#         json.dump(entry, f)\n",
    "#         f.write('\\n')\n",
    "\n",
    "# with open('assets/TEST_CONTEXT.jsonl', 'w') as f:\n",
    "#     for entry in TEST_CONTEXT:\n",
    "#         json.dump(entry, f)\n",
    "#         f.write('\\n')\n",
    "\n",
    "with open('assets/UNSEEN_DATA.jsonl', 'w') as f:\n",
    "    for entry in UNSEEN_DATA:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k-W4UdARgRf"
   },
   "source": [
    "### 3.5 Load, preprocess and save (to JSONL) the CSV data in batches"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uePVdIfvRfn8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "532c1c4b-be1f-4e5b-9261-57347c894d9f"
   },
   "source": [
    "from pandas import DataFrame\n",
    "from spacy.util import minibatch\n",
    "import json\n",
    "\n",
    "# Create assets directory if it doesn't already exist\n",
    "if not os.path.exists(\"assets\"):\n",
    "    os.makedirs(\"assets\")\n",
    "\n",
    "# Read the CSV file as Pandas df\n",
    "col_names = ['text', 'entities']\n",
    "data = pd.read_csv(TEST_CONTEXT_DATA_PATH, names=col_names, usecols=[0, 1])\n",
    "\n",
    "# Shuffle the whole train data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Calulate size of each of the `div` batches\n",
    "tot_size = len(data)\n",
    "div = 4\n",
    "num_groups = int(tot_size / div)\n",
    "print(f\"Size of each part: {num_groups}\\n\")\n",
    "\n",
    "# Divide the data into batches\n",
    "entity_list = data.entities.to_list()\n",
    "entity_batches = minibatch(entity_list, size=num_groups)\n",
    "data_batches = minibatch(data.values.tolist(), size=num_groups)\n",
    "\n",
    "# Process each batch one by one, and save its result in a seperate jsonl file\n",
    "for count, (entity_batch, data_batch) in enumerate(zip(entity_batches, data_batches)):\n",
    "    # if count < 10:\n",
    "    #     # Continue from the desired last batch\n",
    "    #     continue\n",
    "\n",
    "    # Convert the data_batches back to Pandas\n",
    "    data_df = DataFrame(data_batch, columns=col_names)\n",
    "\n",
    "    TRAIN_DATA = load_cleaned_data(data_path=TRAIN_DATA_PATH,\n",
    "                                   train_data_only=entity_batch,\n",
    "                                   train_data_pd=data_df)\n",
    "\n",
    "    with open(f\"assets/TEST_CONTEXT{count}.jsonl\", 'w') as f:\n",
    "        for entry in TRAIN_DATA:\n",
    "            json.dump(entry, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print(f\"Batch {count} procesed and saved.\")\n",
    "    \n",
    "    del TRAIN_DATA\n",
    "    del data_df\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Size of each part: 38584\n",
      "\n",
      "Batch 0 procesed and saved.\n",
      "Batch 1 procesed and saved.\n",
      "Batch 2 procesed and saved.\n",
      "Batch 3 procesed and saved.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j15GJ2cBpxTO"
   },
   "source": [
    "# !!! Forcefully reset RAM by injecting a list of size 10^10 !!!\n",
    "[1]*10**10"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jza6xEbE5ofN"
   },
   "source": [
    "### 3.6 Zip/Unzip the JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sdsTfX-vqBd3"
   },
   "source": [
    "!zip -r /content/assets.zip /content/assets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AQf0WHMtsGCy"
   },
   "source": [
    "!unzip /content/assets.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h9qaV3Blx5M"
   },
   "source": [
    "## 4. Mount Google Drive for access to files\n",
    "\n",
    "* All the processed jsonl files are stored in a folder in Google Drive\n",
    "\n",
    "```\n",
    "MyDrive\n",
    "    └───spacy_ner_data\n",
    "        ├───augmented_dataset_2021-06-30\n",
    "        │   ├───processed_jsonl_files\n",
    "        │   └───processed_spacy_files\n",
    "        └───models\n",
    "            ├───model_both\n",
    "            ├───model_content\n",
    "            └───model_context\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z0r0clYalvSQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "15789515-17b8-4e81-92f6-a54d6ba7930c"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jvWOqtpN9rT"
   },
   "source": [
    "## 5. Convert the data to spaCy's binary format\n",
    "\n",
    "A shell script is made in order to run the preprocess Python script multiple times, iterating over all the jsonl files of dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t1EhwYM5VpOE"
   },
   "source": [
    "%%shell\n",
    "\n",
    "mkdir -p corpus\n",
    "\n",
    "drive_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_jsonl_files/TEST_CONTENT\"\n",
    "# drive_path=\"/content/assets/UNSEEN_DATA\"\n",
    "jsonl_ext=\".jsonl\"\n",
    "\n",
    "saved_path=\"/content/corpus/TEST_CONTENT\"\n",
    "spacy_ext=\".spacy\"\n",
    "\n",
    "for file_iter in {0..3}\n",
    "do\n",
    "    jsonl_drive_path=\"$drive_path$file_iter$jsonl_ext\"\n",
    "    spacy_file_path=\"$saved_path$file_iter$spacy_ext\"\n",
    "\n",
    "    python scripts/preprocess.py \"$jsonl_drive_path\" \"$spacy_file_path\"\n",
    "done"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2GUnSa7omEay"
   },
   "source": [
    "!zip -r /content/corpus.zip ./corpus"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UNeJYN8xvtS"
   },
   "source": [
    "### 5.1 Pre-process single jsonl file only"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4lBf5MoxOC3D"
   },
   "source": [
    "# !python -m spacy project run preprocess\n",
    "!python scripts/preprocess.py merged_file_quarter.jsonl corpus/TRAIN_ALL_QUARTER.spacy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xds9LuitNqch"
   },
   "source": [
    "## 6. Check the config file\n",
    "\n",
    "* Cannot check properly with large dataset because of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KVrMv3h_NgNa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5bf2a2d2-2a77-40d0-cadd-8002581beb4a"
   },
   "source": [
    "!python -m spacy debug data configs/config.cfg"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-07-01 20:28:05.745933: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001B[1m\n",
      "============================ Data file validation ============================\u001B[0m\n",
      "^C\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG9jpo3BOOmY"
   },
   "source": [
    "## 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xcH0mZT5OQ_K",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "d6d3257f-4c2f-44a5-d77a-81361861deaf"
   },
   "source": [
    "%%shell\n",
    "\n",
    "train_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TRAIN_DATA0.spacy\"\n",
    "dev_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TEST_CONTENT_CONTEXT0.spacy\"\n",
    "\n",
    "# !python -m spacy project run train\n",
    "python -m spacy train configs/config.cfg --output training/ --paths.train $train_path --paths.dev $dev_path --gpu-id 0"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-07-03 10:27:20.098515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001B[38;5;2m✔ Created output directory: training\u001B[0m\n",
      "\u001B[38;5;4mℹ Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "[2021-07-03 10:27:22,612] [INFO] Set up nlp object from config\n",
      "[2021-07-03 10:27:22,622] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2021-07-03 10:27:22,626] [INFO] Created vocabulary\n",
      "[2021-07-03 10:27:22,626] [INFO] Finished initializing nlp object\n",
      "Downloading: 100% 481/481 [00:00<00:00, 462kB/s]\n",
      "Downloading: 100% 899k/899k [00:00<00:00, 3.43MB/s]\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 2.26MB/s]\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 5.09MB/s]\n",
      "Downloading: 100% 501M/501M [00:08<00:00, 61.3MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2021-07-03 10:31:16,527] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001B[0m\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.0\u001B[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         140.92     55.04    1.13    1.20    1.07    0.01\n",
      "  0     200        8337.08   3616.94   89.52   93.67   85.73    0.90\n",
      "  0     400         357.33   1183.32   92.99   91.95   94.05    0.93\n",
      "  0     600         133.75   1042.16   91.09   89.59   92.63    0.91\n",
      "  0     800         118.18   1128.79   92.73   90.08   95.53    0.93\n",
      "  0    1000          68.67   1220.19   92.55   92.28   92.83    0.93\n",
      "  0    1200          54.13   1262.61   94.42   94.53   94.31    0.94\n",
      "  0    1400          72.55   1458.31   95.04   95.17   94.92    0.95\n",
      "  0    1600          52.60   1701.42   90.53   87.91   93.31    0.91\n",
      "  0    1800          71.66   2041.97   92.87   94.80   91.02    0.93\n",
      "  0    2000          72.78   2343.91   87.68   89.91   85.57    0.88\n",
      "  0    2200         164.36   2819.78   93.22   96.89   89.81    0.93\n",
      "  0    2400          81.39   2994.37   93.47   97.91   89.41    0.93\n",
      "\n",
      "Aborted!\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "CalledProcessError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-9907730b1c05>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_cell_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'shell'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'\\ntrain_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TRAIN_DATA0.spacy\"\\ndev_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TEST_CONTENT_CONTEXT0.spacy\"\\n\\n# !python -m spacy project run train\\npython -m spacy train configs/config.cfg --output training/ --paths.train $train_path --paths.dev $dev_path --gpu-id 0'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mrun_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2115\u001B[0m             \u001B[0mmagic_arg_s\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvar_expand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstack_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2116\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuiltin_trap\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2117\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmagic_arg_s\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcell\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2118\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001B[0m in \u001B[0;36m_shell_cell_magic\u001B[0;34m(args, cmd)\u001B[0m\n\u001B[1;32m    111\u001B[0m   \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_run_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcmd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclear_streamed_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mparsed_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mignore_errors\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m     \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_returncode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001B[0m in \u001B[0;36mcheck_returncode\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreturncode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m       raise subprocess.CalledProcessError(\n\u001B[0;32m--> 139\u001B[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001B[0m\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_repr_pretty_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcycle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint:disable=unused-argument\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command '\ntrain_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TRAIN_DATA0.spacy\"\ndev_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/TEST_CONTENT_CONTEXT0.spacy\"\n\n# !python -m spacy project run train\npython -m spacy train configs/config.cfg --output training/ --paths.train $train_path --paths.dev $dev_path --gpu-id 0' died with <Signals.SIGTERM: 15>."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmWXqLUqCWjv"
   },
   "source": [
    "## 8. Evaluate\n",
    "\n",
    "There are 3 models stored in the `models` folder.\n",
    "\n",
    "* model_both - Training on train data, and evaluation done using both content and context data\n",
    "\n",
    "* model_content - Training on train data, and evaluation done using only the content data\n",
    "\n",
    "* model_context - Training on train data, and evaluation done using only the context data\n",
    "\n",
    "Each of the 3 models have 2 models, the last model, and the best one. It's recommended to test on the `best-model`.\n",
    "\n",
    "```\n",
    "models\n",
    "    ├───model_both\n",
    "    │   └───training\n",
    "    │       ├───model-best\n",
    "    │       │   ├───ner\n",
    "    │       │   ├───transformer\n",
    "    │       │   │   └───model\n",
    "    │       │   └───vocab\n",
    "    │       └───model-last\n",
    "    │           ├───ner\n",
    "    │           ├───transformer\n",
    "    │           │   └───model\n",
    "    │           └───vocab\n",
    "    ├───model_content\n",
    "    │   └───training\n",
    "    │       ├───model-best\n",
    "    │       │   ├───ner\n",
    "    │       │   ├───transformer\n",
    "    │       │   │   └───model\n",
    "    │       │   └───vocab\n",
    "    │       └───model-last\n",
    "    │           ├───ner\n",
    "    │           ├───transformer\n",
    "    │           │   └───model\n",
    "    │           └───vocab\n",
    "    └───model_context\n",
    "        └───training\n",
    "            ├───model-best\n",
    "            │   ├───ner\n",
    "            │   ├───transformer\n",
    "            │   │   └───model\n",
    "            │   └───vocab\n",
    "            └───model-last\n",
    "                ├───ner\n",
    "                ├───transformer\n",
    "                │   └───model\n",
    "                └───vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S8_FVJOfCQRx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "42236b98-2bcd-4be2-9115-f1d9f27b2d6f"
   },
   "source": [
    "%%shell\n",
    "\n",
    "# For displacy's HTML rendering of annotated outputs\n",
    "# unseen_data_size controls how many examples to annotate in the HTML file,\n",
    "# so it can be less than the actual number of examples in the test data\n",
    "unseen_data_size=1696\n",
    "mkdir -p displacy\n",
    "\n",
    "# Unseen data path (aka data to be tested/evaluated with stored model)\n",
    "test_unseen_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-30/processed_spacy_files/UNSEEN_DATA.spacy\"\n",
    "\n",
    "# Path to the stored model\n",
    "model_path=\"/content/gdrive/MyDrive/spacy_ner_data/models/model_both/training/model-best\"\n",
    "\n",
    "python -m spacy evaluate $model_path $test_unseen_path --output metrics.json --displacy-path displacy --displacy-limit $unseen_data_size --gpu-id 0"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-07-04 14:19:56.223983: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001B[38;5;4mℹ Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "================================== Results ==================================\u001B[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   86.77 \n",
      "NER R   78.71 \n",
      "NER F   82.54 \n",
      "SPEED   6753  \n",
      "\n",
      "\u001B[1m\n",
      "=============================== NER (per type) ===============================\u001B[0m\n",
      "\n",
      "            P       R       F\n",
      "QLTY    79.91   62.94   70.42\n",
      "INSTR   93.48   99.59   96.44\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n",
      "\u001B[38;5;2m✔ Generated 1696 parses as HTML\u001B[0m\n",
      "displacy\n",
      "\u001B[38;5;2m✔ Saved results to metrics.json\u001B[0m\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uYBIJwt5pl2"
   },
   "source": [
    "## 9. Archive the generated model/data/images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uyDleMAd8qnS"
   },
   "source": [
    "# !unzip /content/data.zip\n",
    "# !unzip /content/saved_model.zip\n",
    "# !zip -r /content/data.zip /content/data\n",
    "# !zip -r /content/img.zip /content/img\n",
    "# !zip -r /content/saved_model.zip /content/saved_model\n",
    "# !zip -r /content/training.zip /content/training\n",
    "!unzip /content/training.zip"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}