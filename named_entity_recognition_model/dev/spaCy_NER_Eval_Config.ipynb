{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Config.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dYR2u9U0_JLB",
        "rZDk28NRC_s8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of:\n",
        "    * Enormous dataset size\n",
        "    * Transformer based architecture involving GPU usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0JtYDT_Dx7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fcd5eb-ab65-4f82-d452-2be97aa06e9b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  1 16:39:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBp28TAJmhB"
      },
      "source": [
        "## Install torch\n",
        "\n",
        "* Install torch specifc to the Google Colab's CUDA version\n",
        "* CUDA version 11.1 works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LAGZmbvE0VQ"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WzjtepqIlx2"
      },
      "source": [
        "## Extract Project files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4cI8yUIscq"
      },
      "source": [
        "!unzip /content/project.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZDk28NRC_s8"
      },
      "source": [
        "## Pre-process and save to json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyNJ16vJ5Xov"
      },
      "source": [
        "### Extract the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wph-DdQFKPnO"
      },
      "source": [
        "!unzip /content/augmented_dataset_2021-06-30.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qCBw9tx5f9R"
      },
      "source": [
        "### Loader function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjtHcwhDMYD"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "\n",
        "TRAIN_DATA_PATH = \"./augmented_dataset_2021-06-30/train.csv\"\n",
        "TEST_CONTENT_DATA_PATH = \"./augmented_dataset_2021-06-30/test_content.csv\"\n",
        "TEST_CONTEXT_DATA_PATH = \"./augmented_dataset_2021-06-30/test_context.csv\"\n",
        "TEST_UNSEEN = \"./augmented_dataset_2021-06-30/test_unseen.csv\"\n",
        "\n",
        "def load_cleaned_data(data_path, train_data_only=None, train_data_pd=None):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    if train_data_only is None:\n",
        "        col_names = ['text', 'entities']\n",
        "\n",
        "        data = pd.read_csv(data_path, names=col_names, usecols=[0, 1])\n",
        "        entity_list = data.entities.to_list()\n",
        "\n",
        "    else:\n",
        "        # Incoming `train_data_only` is itself a pandas,\n",
        "        # so just process it.\n",
        "        entity_list = train_data_only\n",
        "        data = train_data_pd\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"tokens\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        tokens_list = []\n",
        "        spans_list = []\n",
        "\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            word_pair_list = []\n",
        "            \n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "\n",
        "            # Remove any leading or beginning blank space\n",
        "            word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "            start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "            start_index = int(start_index + 0)\n",
        "            end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "            # Incase word not found in the sentence\n",
        "            if start_index == -1:\n",
        "                print(\"\\n-1 error\")\n",
        "                print(\"Couldn't find:\")\n",
        "                print(word_pair_list[0])\n",
        "                print(\"in:\")\n",
        "                print(data['text'][index])\n",
        "                break\n",
        "\n",
        "            both_present = lambda: (start_index, end_index) in indices_list\n",
        "            start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "            end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "            left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "            def right_blank():\n",
        "                # return true if there is no blank space after the end_index,\n",
        "                # as long as end_index is not at the end of the sentence\n",
        "                if len(data['text'][index].lower()) != end_index:\n",
        "                    return data['text'][index][end_index] != \" \"\n",
        "            \n",
        "            # Check if this start_index and/or end_index is already in the list:\n",
        "            # (To prevent overlapping with already tagged words)\n",
        "            flag = 0\n",
        "            while True:\n",
        "                if (start_index == -1 or end_index == -1):\n",
        "                    flag = 1\n",
        "                    break\n",
        "                if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                \n",
        "                    start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                        start=end_index + 1).astype(numpy.int64)\n",
        "                    start_index = int(start_index + 0)\n",
        "                    end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "                else:\n",
        "                    indices_list.append((start_index, end_index))\n",
        "                    break\n",
        "            \n",
        "            if (flag == 1):\n",
        "                # Don't bother checking rest of the current sentence\n",
        "                break\n",
        "            \n",
        "            # Add ALL the words and their positions to a \"tokens\" list\n",
        "            tokens_list.append({\"text\": word_pair_list[0], \"start\": start_index, \"end\": end_index})\n",
        "\n",
        "            # Add the specially tagged words to a \"spans\" list\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "                spans_list.append({\"start\": start_index, \"end\": end_index, \"label\": word_pair_list[1]})\n",
        "\n",
        "        DATA.append({\"text\": data['text'][index].lower(), \"tokens\": tokens_list, \"spans\": spans_list, \"answer\": \"accept\"})\n",
        "        \n",
        "    return DATA\n",
        "\n",
        "\n",
        "# TRAIN_DATA = load_cleaned_data(TRAIN_DATA_PATH)\n",
        "# TEST_CONTENT = load_cleaned_data(TEST_CONTENT_DATA_PATH)\n",
        "# TEST_CONTEXT = load_cleaned_data(TEST_CONTEXT_DATA_PATH)\n",
        "UNSEEN_DATA = load_cleaned_data(TEST_UNSEEN)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k-W4UdARgRf"
      },
      "source": [
        "### Load, preprocess and save (to JSONL) the CSV data in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uePVdIfvRfn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532c1c4b-be1f-4e5b-9261-57347c894d9f"
      },
      "source": [
        "from pandas import DataFrame\n",
        "from spacy.util import minibatch\n",
        "import json\n",
        "\n",
        "# Create assets directory if it doesn't already exist\n",
        "if not os.path.exists(\"assets\"):\n",
        "    os.makedirs(\"assets\")\n",
        "\n",
        "# Read the CSV file as Pandas df\n",
        "col_names = ['text', 'entities']\n",
        "data = pd.read_csv(TEST_CONTEXT_DATA_PATH, names=col_names, usecols=[0, 1])\n",
        "\n",
        "# Shuffle the whole train data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Calulate size of each of the `div` batches\n",
        "tot_size = len(data)\n",
        "div = 4\n",
        "num_groups = int(tot_size / div)\n",
        "print(f\"Size of each part: {num_groups}\\n\")\n",
        "\n",
        "# Divide the data into batches\n",
        "entity_list = data.entities.to_list()\n",
        "entity_batches = minibatch(entity_list, size=num_groups)\n",
        "data_batches = minibatch(data.values.tolist(), size=num_groups)\n",
        "\n",
        "# Process each batch one by one, and save its result in a seperate jsonl file\n",
        "for count, (entity_batch, data_batch) in enumerate(zip(entity_batches, data_batches)):\n",
        "    # if count < 10:\n",
        "    #     # Continue from the desired last batch\n",
        "    #     continue\n",
        "\n",
        "    # Convert the data_batches back to Pandas\n",
        "    data_df = DataFrame(data_batch, columns=col_names)\n",
        "\n",
        "    TRAIN_DATA = load_cleaned_data(data_path=TRAIN_DATA_PATH,\n",
        "                                   train_data_only=entity_batch,\n",
        "                                   train_data_pd=data_df)\n",
        "\n",
        "    with open(f\"assets/TEST_CONTEXT{count}.jsonl\", 'w') as f:\n",
        "        for entry in TRAIN_DATA:\n",
        "            json.dump(entry, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"Batch {count} procesed and saved.\")\n",
        "    \n",
        "    del TRAIN_DATA\n",
        "    del data_df\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of each part: 38584\n",
            "\n",
            "Batch 0 procesed and saved.\n",
            "Batch 1 procesed and saved.\n",
            "Batch 2 procesed and saved.\n",
            "Batch 3 procesed and saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljvb--hbrcK"
      },
      "source": [
        "# Clear the assets folder\n",
        "! rm -r assets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15GJ2cBpxTO"
      },
      "source": [
        "# !!! Forcefully reset RAM by injecting a list of size 10^10 !!!\n",
        "[1]*10**10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaFqxUFg5jTv"
      },
      "source": [
        "### Save to JSONL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QamFbSfcXJT"
      },
      "source": [
        "import json\n",
        "\n",
        "if not os.path.exists(\"assets\"):\n",
        "        os.makedirs(\"assets\")\n",
        "\n",
        "# with open('assets/TRAIN_DATA.jsonl', 'w') as f:\n",
        "#     for entry in TRAIN_DATA:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/TEST_CONTENT.jsonl', 'w') as f:\n",
        "#     for entry in TEST_CONTENT:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/TEST_CONTEXT.jsonl', 'w') as f:\n",
        "#     for entry in TEST_CONTEXT:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "with open('assets/UNSEEN_DATA.jsonl', 'w') as f:\n",
        "    for entry in UNSEEN_DATA:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jza6xEbE5ofN"
      },
      "source": [
        "### Zip the JSONL files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsTfX-vqBd3"
      },
      "source": [
        "!zip -r /content/assets.zip /content/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auxp_oSxUe2u"
      },
      "source": [
        "## Extract assets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQf0WHMtsGCy"
      },
      "source": [
        "!unzip /content/assets.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h9qaV3Blx5M"
      },
      "source": [
        "## Get the pre-processed JSONL dataset from Google Drive\n",
        "\n",
        "The below cell joins the jsonl files, but **does not format them properly**.  \n",
        "Probably not gonna be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0r0clYalvSQ"
      },
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "TRAIN = []\n",
        "\n",
        "for file_iter, f in enumerate(glob.glob(\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-21/train_jsonl_files/shuffled/*.jsonl\")):\n",
        "    with open(f, \"rb\") as infile:\n",
        "        json_list = list(infile)\n",
        "        TRAIN.append(json_list)\n",
        "    print(f\"File {file_iter} appended.\")\n",
        "\n",
        "with open(\"merged_file_10th.jsonl\", \"w\") as outfile:\n",
        "     json.dump(TRAIN[0:198522], outfile)\n",
        "\n",
        "\n",
        "# read_files = glob.glob(\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-21/train_jsonl_files/shuffled/*.jsonl\")\n",
        "# with open(\"merged_file.jsonl\", \"wb\") as outfile:\n",
        "#     outfile.write('[{}]'.format(\n",
        "#         b','.join([open(f, \"rb\").read() for f in read_files])))\n",
        "\n",
        "# for file_iter in range(100 + 1):\n",
        "#     BIG_DATA_PATH = f\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-21/train_jsonl_files/shuffled/TRAIN_DATA{file_iter}.jsonl\"\n",
        "#     with open(BIG_DATA_PATH, 'r') as f:\n",
        "#         TRAIN.append(json.load(f))\n",
        "#     print(f\"File {file_iter} appended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWnKdnlY24ee"
      },
      "source": [
        "del TRAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOd_YEeE1r-b"
      },
      "source": [
        "!zip -r /merged_file_quarter.zip /content/merged_file_quarter.jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jvWOqtpN9rT"
      },
      "source": [
        "## Convert the data to spaCy's binary format\n",
        "\n",
        "A shell script is made in order to run the preprocess Python script multiple times, iterating over all the 100 jsonl files of training dataset\n",
        "\n",
        "* The error logs generated by spaCy point to 11 sentences, each having 100 duplicates in the original train.csv file.\n",
        "* The actual reason of these spaCy errors are not duplicates but the inability of the preprocessing function (`load_cleaned_data`) to identiy tagged INTR and/or QLTY of very few specific sentences (To be fixed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1EhwYM5VpOE",
        "outputId": "e2b10a81-fb1e-4beb-878d-f1444a9d6e27"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p corpus\n",
        "\n",
        "# drive_path=\"/content/gdrive/MyDrive/spacy_ner_data/augmented_dataset_2021-06-21/train_jsonl_files/shuffled/TRAIN_DATA\"\n",
        "drive_path=\"/content/assets/UNSEEN_DATA\"\n",
        "jsonl_ext=\".jsonl\"\n",
        "\n",
        "saved_path=\"/content/corpus/UNSEEN_DATA\"\n",
        "spacy_ext=\".spacy\"\n",
        "\n",
        "for file_iter in {0..0}\n",
        "do\n",
        "    jsonl_drive_path=\"$drive_path$file_iter$jsonl_ext\"\n",
        "    spacy_file_path=\"$saved_path$file_iter$spacy_ext\"\n",
        "\n",
        "    python scripts/preprocess.py \"$jsonl_drive_path\" \"$spacy_file_path\"\n",
        "done"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-01 20:44:56.155564: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Processed 1696 documents: UNSEEN_DATA0.spacy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GUnSa7omEay"
      },
      "source": [
        "!zip -r /content/corpus.zip ./corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvUJx-2E8Vqx"
      },
      "source": [
        "!unzip /content/corpus.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBf5MoxOC3D"
      },
      "source": [
        "# !python -m spacy project run preprocess\n",
        "!python scripts/preprocess.py merged_file_quarter.jsonl corpus/TRAIN_ALL_QUARTER.spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xds9LuitNqch"
      },
      "source": [
        "## Check the config file\n",
        "\n",
        "* Cannot check properly with large dataset because of memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrMv3h_NgNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf2a2d2-2a77-40d0-cadd-8002581beb4a"
      },
      "source": [
        "!python -m spacy debug data configs/config.cfg"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-01 20:28:05.745933: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9jpo3BOOmY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcH0mZT5OQ_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9192bba-289a-41e3-d229-bc57ee5a817a"
      },
      "source": [
        "# !python -m spacy project run train\n",
        "!python -m spacy train configs/config.cfg --output training/ --paths.train corpus/TRAIN_DATA0.spacy --paths.dev corpus/TEST_CONTENT0.spacy --gpu-id 0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-01 19:45:30.049450: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-07-01 19:45:33,057] [INFO] Set up nlp object from config\n",
            "[2021-07-01 19:45:33,067] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2021-07-01 19:45:33,071] [INFO] Created vocabulary\n",
            "[2021-07-01 19:45:33,071] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2021-07-01 19:49:08,312] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         140.92     55.04    2.07    2.10    2.03    0.02\n",
            "  0     200        7824.79   3617.41   88.25   88.54   87.96    0.88\n",
            "  0     400         465.93   1373.36   86.56   82.47   91.08    0.87\n",
            "  0     600         523.81   1592.54   86.56   82.47   91.08    0.87\n",
            "  0     800         627.11   1909.86   86.56   82.47   91.08    0.87\n",
            "  0    1000         712.32   2225.56   86.56   82.47   91.08    0.87\n",
            "  0    1200         839.03   2552.22   86.56   82.47   91.08    0.87\n",
            "  0    1400        1020.75   3062.75   86.56   82.47   91.08    0.87\n",
            "  0    1600        1206.99   3809.29   86.56   82.47   91.08    0.87\n",
            "  0    1800        1536.50   4804.61   86.56   82.47   91.08    0.87\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/model-last\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWXqLUqCWjv"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8_FVJOfCQRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4480c2d8-c369-4314-e1d1-bb504a123a1f"
      },
      "source": [
        "# !python -m spacy project run evaluate\n",
        "!mkdir -p displacy\n",
        "!python -m spacy evaluate training/model-best corpus/UNSEEN_DATA0.spacy --output training/metrics.json --gpu-id 0 --displacy-path displacy"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-01 21:04:49.032978: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   83.22 \n",
            "NER R   80.44 \n",
            "NER F   81.81 \n",
            "SPEED   7146  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "            P       R       F\n",
            "QLTY    74.00   73.04   73.52\n",
            "INSTR   96.04   90.24   93.05\n",
            "\n",
            "\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n",
            "displacy\n",
            "\u001b[38;5;2m✔ Saved results to training/metrics.json\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "# !unzip /content/saved_model.zip\n",
        "# !zip -r /content/data.zip /content/data\n",
        "# !zip -r /content/img.zip /content/img\n",
        "# !zip -r /content/saved_model.zip /content/saved_model\n",
        "!zip -r /content/training.zip /content/training"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}