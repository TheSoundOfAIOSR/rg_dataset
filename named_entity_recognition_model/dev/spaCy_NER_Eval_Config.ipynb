{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Config.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dYR2u9U0_JLB",
        "rZDk28NRC_s8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of:\n",
        "    * Enormous dataset size\n",
        "    * Transformer based architecture involving GPU usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0JtYDT_Dx7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d32975-9cd7-42f6-eb62-5096879f7c32"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 27 00:07:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBp28TAJmhB"
      },
      "source": [
        "## Install torch\n",
        "\n",
        "* Install torch specifc to the Google Colab's CUDA version\n",
        "* CUDA version 11.1 works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LAGZmbvE0VQ"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WzjtepqIlx2"
      },
      "source": [
        "## Extract Project files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4cI8yUIscq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2168304a-c334-4565-bf94-b926aed50833"
      },
      "source": [
        "!unzip /content/project.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/project.zip\n",
            " extracting: .gitignore              \n",
            "   creating: configs/\n",
            "  inflating: configs/config.cfg      \n",
            "  inflating: project.lock            \n",
            "  inflating: project.yml             \n",
            "  inflating: README.md               \n",
            " extracting: requirements.txt        \n",
            "   creating: scripts/\n",
            "  inflating: scripts/preprocess.py   \n",
            "  inflating: scripts/visualize_data.py  \n",
            "  inflating: scripts/visualize_model.py  \n",
            "  inflating: test_project_ner_fashion_brands.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZDk28NRC_s8"
      },
      "source": [
        "## Pre-process and save to json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyNJ16vJ5Xov"
      },
      "source": [
        "### Extract the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wph-DdQFKPnO",
        "outputId": "8ab807c6-c623-4d9f-ae35-2d58a5032977"
      },
      "source": [
        "!unzip /content/augmented_dataset_2021-06-21.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/augmented_dataset_2021-06-21.zip\n",
            "   creating: augmented_dataset_2021-06-21/\n",
            "  inflating: augmented_dataset_2021-06-21/keyword_ids.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/pattern_ids.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_content.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_context.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_unseen.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qCBw9tx5f9R"
      },
      "source": [
        "### Loader function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjtHcwhDMYD"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "\n",
        "TRAIN_DATA_PATH = \"./augmented_dataset_2021-06-21/train.csv\"\n",
        "TEST_CONTENT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_content.csv\"\n",
        "TEST_CONTEXT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_context.csv\"\n",
        "TEST_UNSEEN = \"./augmented_dataset_2021-06-21/test_unseen.csv\"\n",
        "\n",
        "def load_cleaned_data(data_path):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    col_names = ['text', 'entities']\n",
        "\n",
        "    data = pd.read_csv(data_path, names=col_names, usecols=[0, 1])\n",
        "    entity_list = data.entities.to_list()\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"tokens\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        tokens_list = []\n",
        "        spans_list = []\n",
        "\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "\n",
        "            # Remove any leading or beginning blank space\n",
        "            word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "            start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "            start_index = int(start_index + 0)\n",
        "            end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "            # Incase word not found in the sentence\n",
        "            if start_index == -1:\n",
        "                print(\"-1 error\")\n",
        "                print(data['text'][index])\n",
        "                break\n",
        "\n",
        "            both_present = lambda: (start_index, end_index) in indices_list\n",
        "            start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "            end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "            left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "            def right_blank():\n",
        "                # return true if there is no blank space after the end_index,\n",
        "                # as long as end_index is not at the end of the sentence\n",
        "                if len(data['text'][index].lower()) != end_index:\n",
        "                    return data['text'][index][end_index] != \" \"\n",
        "            \n",
        "            # Check if this start_index and/or end_index is already in the list:\n",
        "            # (To prevent overlapping with already tagged words)\n",
        "            flag = 0\n",
        "            while True:\n",
        "                if (start_index == -1 or end_index == -1):\n",
        "                    flag = 1\n",
        "                    break\n",
        "                if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                \n",
        "                    start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                        start=end_index + 1).astype(numpy.int64)\n",
        "                    start_index = int(start_index + 0)\n",
        "                    end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "                else:\n",
        "                    indices_list.append((start_index, end_index))\n",
        "                    break\n",
        "            \n",
        "            if (flag == 1):\n",
        "                # Don't bother checking rest of the current sentence\n",
        "                break\n",
        "            \n",
        "            # Add ALL the words and their positions to a \"tokens\" list\n",
        "            tokens_list.append({\"text\": word_pair_list[0], \"start\": start_index, \"end\": end_index})\n",
        "\n",
        "            # Add the specially tagged words to a \"spans\" list\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "                spans_list.append({\"start\": start_index, \"end\": end_index, \"label\": word_pair_list[1]})\n",
        "\n",
        "        DATA.append({\"text\": data['text'][index].lower(), \"tokens\": tokens_list, \"spans\": spans_list, \"answer\": \"accept\"})\n",
        "        \n",
        "    return DATA\n",
        "\n",
        "\n",
        "# TRAIN_DATA = load_cleaned_data(TRAIN_DATA_PATH)\n",
        "# TEST_CONTENT = load_cleaned_data(TEST_CONTENT_DATA_PATH)\n",
        "TEST_CONTEXT = load_cleaned_data(TEST_CONTEXT_DATA_PATH)\n",
        "# UNSEEN_DATA = load_cleaned_data(TEST_UNSEEN)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaFqxUFg5jTv"
      },
      "source": [
        "### Save to JSONL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QamFbSfcXJT"
      },
      "source": [
        "import json\n",
        "if not os.path.exists(\"assets\"):\n",
        "        os.makedirs(\"assets\")\n",
        "\n",
        "# with open('assets/TRAIN_DATA.jsonl', 'w') as f:\n",
        "#     for entry in TRAIN_DATA:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/TEST_CONTENT.jsonl', 'w') as f:\n",
        "#     for entry in TEST_CONTENT:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "with open('assets/TEST_CONTEXT.jsonl', 'w') as f:\n",
        "    for entry in TEST_CONTEXT:\n",
        "        json.dump(entry, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "# with open('assets/UNSEEN_DATA.jsonl', 'w') as f:\n",
        "#     for entry in UNSEEN_DATA:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jza6xEbE5ofN"
      },
      "source": [
        "### Zip the JSONL files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsTfX-vqBd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a75bb5e-46eb-4fad-8ca6-9f9da87bae56"
      },
      "source": [
        "!zip -r /content/assets.zip /content/assets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/assets/ (stored 0%)\n",
            "  adding: content/assets/TEST_CONTEXT.jsonl (deflated 94%)\n",
            "  adding: content/assets/TEST_CONTENT.jsonl (deflated 96%)\n",
            "  adding: content/assets/.ipynb_checkpoints/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auxp_oSxUe2u"
      },
      "source": [
        "## Extract assets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQf0WHMtsGCy",
        "outputId": "eab66973-c3cf-4e41-ed2c-79cdfa178d43"
      },
      "source": [
        "!unzip /content/assets.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/assets.zip\n",
            "   creating: assets/\n",
            "  inflating: assets/TEST_CONTENT.jsonl  \n",
            "  inflating: assets/TEST_CONTEXT.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jvWOqtpN9rT"
      },
      "source": [
        "## Convert the data to spaCy's binary format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBf5MoxOC3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0479d374-2a01-4552-8d56-d49aeb02b4f8"
      },
      "source": [
        "!python -m spacy project run preprocess"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-27 00:08:27.911511: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/preprocess.py assets/TEST_CONTEXT.jsonl corpus/TEST_CONTEXT.spacy\n",
            "2021-06-27 00:08:32.140911: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Processed 207134 documents: TEST_CONTEXT.spacy\n",
            "Running command: /usr/bin/python3 scripts/preprocess.py assets/TEST_CONTENT.jsonl corpus/TEST_CONTENT.spacy\n",
            "2021-06-27 00:10:47.654556: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Processed 19608 documents: TEST_CONTENT.spacy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xds9LuitNqch"
      },
      "source": [
        "## Check the config file\n",
        "\n",
        "* Cannot check properly with large dataset because of memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVrMv3h_NgNa",
        "outputId": "11e23128-f72a-4263-c0bf-ca6c2348c885"
      },
      "source": [
        "!python -m spacy debug data configs/config.cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-27 00:11:14.299376: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9jpo3BOOmY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcH0mZT5OQ_K",
        "outputId": "f960cd7c-d88a-417a-d1a5-d532b34acdee"
      },
      "source": [
        "# !python -m spacy project run train\n",
        "!python -m spacy train configs/config.cfg --output training/ --paths.train corpus/TEST_CONTEXT.spacy --paths.dev corpus/TEST_CONTENT.spacy --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-27 00:51:22.690213: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-06-27 00:51:28,547] [INFO] Set up nlp object from config\n",
            "[2021-06-27 00:51:28,557] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2021-06-27 00:51:28,561] [INFO] Created vocabulary\n",
            "[2021-06-27 00:51:28,561] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2021-06-27 01:00:03,211] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         461.49     92.52    1.13    0.94    1.42    0.01\n",
            "  0     200        8347.67   4124.20   77.47   82.01   73.40    0.77\n",
            "  0     400         264.08    837.67   84.16   83.40   84.93    0.84\n",
            "  0     600         101.89    652.80   74.90   64.36   89.57    0.75\n",
            "  0     800          65.42    627.16   79.83   74.87   85.49    0.80\n",
            "  0    1000          57.82    625.70   82.30   74.60   91.76    0.82\n",
            "  0    1200          25.84    701.15   78.28   70.18   88.47    0.78\n",
            "  0    1400          34.26    732.95   76.27   66.37   89.65    0.76\n",
            "  0    1600          49.35    897.71   80.15   72.67   89.36    0.80\n",
            "  0    1800          23.10    874.70   84.70   85.12   84.27    0.85\n",
            "  0    2000          24.50    981.47   77.14   69.22   87.10    0.77\n",
            "Epoch 1: 100% 199/200 [01:34<00:00,  2.05it/s]^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWXqLUqCWjv"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8_FVJOfCQRx"
      },
      "source": [
        "# !python -m spacy project run evaluate\n",
        "!python -m spacy evaluate training/model-best corpus/fashion_brands_eval.spacy --output training/metrics.json --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "# !unzip /content/saved_model.zip\n",
        "# !zip -r /content/data.zip /content/data\n",
        "# !zip -r /content/img.zip /content/img\n",
        "# !zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}