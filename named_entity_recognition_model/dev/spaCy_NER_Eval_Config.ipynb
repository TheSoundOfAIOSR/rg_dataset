{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_NER_Eval_Config.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dYR2u9U0_JLB",
        "rZDk28NRC_s8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQUUODx7h4v"
      },
      "source": [
        "# NER Evaluation of Augmented data\n",
        "\n",
        "* This evaluation is done in Google Colab because of:\n",
        "    * Enormous dataset size\n",
        "    * Transformer based architecture involving GPU usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYR2u9U0_JLB"
      },
      "source": [
        "## Install spaCy and download English model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBO4JulnpP7"
      },
      "source": [
        "# !pip install cupy-cuda112\n",
        "!pip install spacy==3.0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJC5o7XpHOs"
      },
      "source": [
        "# Download spacy small model\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0JtYDT_Dx7S"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBp28TAJmhB"
      },
      "source": [
        "## Install torch\n",
        "\n",
        "* Install torch specifc to the Google Colab's CUDA version\n",
        "* CUDA version 11.1 works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LAGZmbvE0VQ"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WzjtepqIlx2"
      },
      "source": [
        "## Extract Project files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4cI8yUIscq"
      },
      "source": [
        "!unzip /content/project.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZDk28NRC_s8"
      },
      "source": [
        "## Pre-process and save to json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyNJ16vJ5Xov"
      },
      "source": [
        "### Extract the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wph-DdQFKPnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba24c550-8913-4ad6-b694-282ab1ad44bd"
      },
      "source": [
        "!unzip /content/augmented_dataset_2021-06-21.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/augmented_dataset_2021-06-21.zip\n",
            "   creating: augmented_dataset_2021-06-21/\n",
            "  inflating: augmented_dataset_2021-06-21/keyword_ids.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/pattern_ids.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_content.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_context.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/test_unseen.csv  \n",
            "  inflating: augmented_dataset_2021-06-21/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qCBw9tx5f9R"
      },
      "source": [
        "### Loader function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDjtHcwhDMYD"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy\n",
        "from numpy.core.defchararray import find\n",
        "\n",
        "TRAIN_DATA_PATH = \"./augmented_dataset_2021-06-21/train.csv\"\n",
        "TEST_CONTENT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_content.csv\"\n",
        "TEST_CONTEXT_DATA_PATH = \"./augmented_dataset_2021-06-21/test_context.csv\"\n",
        "TEST_UNSEEN = \"./augmented_dataset_2021-06-21/test_unseen.csv\"\n",
        "\n",
        "def load_cleaned_data(data_path, train_data_only=None, train_data_pd=None):\n",
        "    \"\"\"\n",
        "    Go through every sentence's all word-tag pair (except \"NONE\")\n",
        "    and calculate the start and end index.\n",
        "    After getting the (start, end) pair, check if this pair was already calculated\n",
        "    (i.e., either the start_index, OR end_index, OR both are matching with the ones in list),\n",
        "    and if so, discard the pair and continue calculating again, skipping over the one discarded.\n",
        "    :return: DATA\n",
        "    \"\"\"\n",
        "    if train_data_only is None:\n",
        "        col_names = ['text', 'entities']\n",
        "\n",
        "        data = pd.read_csv(data_path, names=col_names, usecols=[0, 1])\n",
        "        entity_list = data.entities.to_list()\n",
        "\n",
        "    else:\n",
        "        # Incoming `train_data_only` is itself a pandas,\n",
        "        # so just process it.\n",
        "        entity_list = train_data_only\n",
        "        data = train_data_pd\n",
        "\n",
        "    DATA = []\n",
        "\n",
        "    for index, ent in enumerate(entity_list):\n",
        "        if ent == \"tokens\":\n",
        "            continue\n",
        "\n",
        "        ent = ent.split(\"), (\")\n",
        "        ent[0] = re.sub(\"[([]\", \"\", ent[0])\n",
        "        ent[-1] = re.sub(\"[)]]\", \"\", ent[-1])\n",
        "\n",
        "        # Initialize index list, to store pairs of (start, end) indices\n",
        "        indices_list = [(-1, -1), (-1, -1)]\n",
        "\n",
        "        tokens_list = []\n",
        "        spans_list = []\n",
        "\n",
        "        start_index = 0\n",
        "        end_index = 0\n",
        "\n",
        "        # Analyze current \"split_sentences\"'s all word-pairs\n",
        "        for index_ent, word_pair in enumerate(ent):\n",
        "            word_pair_list = []\n",
        "            \n",
        "            # Split the word and its pair\n",
        "            word_pair_list = word_pair.split(\"'\")[1::2]\n",
        "\n",
        "            # Remove any leading or beginning blank space\n",
        "            word_pair_list[0] = word_pair_list[0].strip()\n",
        "\n",
        "            start_index = find(data['text'][index].lower(), word_pair_list[0]).astype(numpy.int64)\n",
        "            start_index = int(start_index + 0)\n",
        "            end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "            # Incase word not found in the sentence\n",
        "            if start_index == -1:\n",
        "                print(\"\\n-1 error\")\n",
        "                print(\"Couldn't find:\")\n",
        "                print(word_pair_list[0])\n",
        "                print(\"in:\")\n",
        "                print(data['text'][index])\n",
        "                break\n",
        "\n",
        "            both_present = lambda: (start_index, end_index) in indices_list\n",
        "            start_present = lambda: start_index in [i[0] for i in indices_list]\n",
        "            end_present = lambda: end_index in [i[1] for i in indices_list]\n",
        "            left_blank = lambda: data['text'][index][start_index - 1] != \" \"\n",
        "\n",
        "            def right_blank():\n",
        "                # return true if there is no blank space after the end_index,\n",
        "                # as long as end_index is not at the end of the sentence\n",
        "                if len(data['text'][index].lower()) != end_index:\n",
        "                    return data['text'][index][end_index] != \" \"\n",
        "            \n",
        "            # Check if this start_index and/or end_index is already in the list:\n",
        "            # (To prevent overlapping with already tagged words)\n",
        "            flag = 0\n",
        "            while True:\n",
        "                if (start_index == -1 or end_index == -1):\n",
        "                    flag = 1\n",
        "                    break\n",
        "                if (both_present()) or (start_present()) or (end_present()) or (left_blank()) or (right_blank()):\n",
        "                \n",
        "                    start_index = find(data['text'][index].lower(), word_pair_list[0],\n",
        "                                        start=end_index + 1).astype(numpy.int64)\n",
        "                    start_index = int(start_index + 0)\n",
        "                    end_index = int(start_index + len(word_pair_list[0]))\n",
        "\n",
        "                else:\n",
        "                    indices_list.append((start_index, end_index))\n",
        "                    break\n",
        "            \n",
        "            if (flag == 1):\n",
        "                # Don't bother checking rest of the current sentence\n",
        "                break\n",
        "            \n",
        "            # Add ALL the words and their positions to a \"tokens\" list\n",
        "            tokens_list.append({\"text\": word_pair_list[0], \"start\": start_index, \"end\": end_index})\n",
        "\n",
        "            # Add the specially tagged words to a \"spans\" list\n",
        "            if word_pair_list[1] != \"NONE\":\n",
        "                spans_list.append({\"start\": start_index, \"end\": end_index, \"label\": word_pair_list[1]})\n",
        "\n",
        "        DATA.append({\"text\": data['text'][index].lower(), \"tokens\": tokens_list, \"spans\": spans_list, \"answer\": \"accept\"})\n",
        "        \n",
        "    return DATA\n",
        "\n",
        "\n",
        "# TRAIN_DATA = load_cleaned_data(TRAIN_DATA_PATH)\n",
        "# TEST_CONTENT = load_cleaned_data(TEST_CONTENT_DATA_PATH)\n",
        "# TEST_CONTEXT = load_cleaned_data(TEST_CONTEXT_DATA_PATH)\n",
        "# UNSEEN_DATA = load_cleaned_data(TEST_UNSEEN)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k-W4UdARgRf"
      },
      "source": [
        "### Load and save `TRAIN_DATA` in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uePVdIfvRfn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5474e8-2f94-4e5e-9fb6-52342d1a99ed"
      },
      "source": [
        "from pandas import DataFrame\n",
        "from spacy.util import minibatch\n",
        "import json\n",
        "\n",
        "# Create assets directory if it doesn't already exist\n",
        "if not os.path.exists(\"assets\"):\n",
        "    os.makedirs(\"assets\")\n",
        "\n",
        "# Read the CSV file as Pandas df\n",
        "col_names = ['text', 'entities']\n",
        "data = pd.read_csv(TRAIN_DATA_PATH, names=col_names, usecols=[0, 1])\n",
        "\n",
        "# Shuffle the whole train data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Calulate size of each of the `div` batches\n",
        "tot_size = len(data)\n",
        "div = 100\n",
        "num_groups = int(tot_size / div)\n",
        "print(f\"Size of each part: {num_groups}\\n\")\n",
        "\n",
        "# Divide the data into batches\n",
        "entity_list = data.entities.to_list()\n",
        "entity_batches = minibatch(entity_list, size=num_groups)\n",
        "data_batches = minibatch(data.values.tolist(), size=num_groups)\n",
        "\n",
        "# Process each batch one by one, and save its result in a seperate jsonl file\n",
        "for count, (entity_batch, data_batch) in enumerate(zip(entity_batches, data_batches)):\n",
        "    # if count < 10:\n",
        "    #     # Continue from the desired last batch\n",
        "    #     continue\n",
        "\n",
        "    # Convert the data_batches back to Pandas\n",
        "    data_df = DataFrame(data_batch, columns=col_names)\n",
        "\n",
        "    TRAIN_DATA = load_cleaned_data(data_path=TRAIN_DATA_PATH,\n",
        "                                   train_data_only=entity_batch,\n",
        "                                   train_data_pd=data_df)\n",
        "\n",
        "    with open(f\"assets/TRAIN_DATA{count}.jsonl\", 'w') as f:\n",
        "        for entry in TRAIN_DATA:\n",
        "            json.dump(entry, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"Batch {count} procesed and saved.\")\n",
        "    \n",
        "    del TRAIN_DATA\n",
        "    del data_df\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of each part: 19852\n",
            "\n",
            "Batch 0 procesed and saved.\n",
            "Batch 1 procesed and saved.\n",
            "Batch 2 procesed and saved.\n",
            "Batch 3 procesed and saved.\n",
            "Batch 4 procesed and saved.\n",
            "Batch 5 procesed and saved.\n",
            "Batch 6 procesed and saved.\n",
            "Batch 7 procesed and saved.\n",
            "Batch 8 procesed and saved.\n",
            "Batch 9 procesed and saved.\n",
            "Batch 10 procesed and saved.\n",
            "Batch 11 procesed and saved.\n",
            "Batch 12 procesed and saved.\n",
            "Batch 13 procesed and saved.\n",
            "Batch 14 procesed and saved.\n",
            "Batch 15 procesed and saved.\n",
            "Batch 16 procesed and saved.\n",
            "Batch 17 procesed and saved.\n",
            "Batch 18 procesed and saved.\n",
            "Batch 19 procesed and saved.\n",
            "Batch 20 procesed and saved.\n",
            "Batch 21 procesed and saved.\n",
            "Batch 22 procesed and saved.\n",
            "Batch 23 procesed and saved.\n",
            "Batch 24 procesed and saved.\n",
            "Batch 25 procesed and saved.\n",
            "Batch 26 procesed and saved.\n",
            "Batch 27 procesed and saved.\n",
            "Batch 28 procesed and saved.\n",
            "Batch 29 procesed and saved.\n",
            "Batch 30 procesed and saved.\n",
            "Batch 31 procesed and saved.\n",
            "Batch 32 procesed and saved.\n",
            "Batch 33 procesed and saved.\n",
            "Batch 34 procesed and saved.\n",
            "Batch 35 procesed and saved.\n",
            "Batch 36 procesed and saved.\n",
            "Batch 37 procesed and saved.\n",
            "Batch 38 procesed and saved.\n",
            "Batch 39 procesed and saved.\n",
            "Batch 40 procesed and saved.\n",
            "Batch 41 procesed and saved.\n",
            "Batch 42 procesed and saved.\n",
            "Batch 43 procesed and saved.\n",
            "Batch 44 procesed and saved.\n",
            "Batch 45 procesed and saved.\n",
            "Batch 46 procesed and saved.\n",
            "Batch 47 procesed and saved.\n",
            "Batch 48 procesed and saved.\n",
            "Batch 49 procesed and saved.\n",
            "Batch 50 procesed and saved.\n",
            "Batch 51 procesed and saved.\n",
            "Batch 52 procesed and saved.\n",
            "Batch 53 procesed and saved.\n",
            "Batch 54 procesed and saved.\n",
            "Batch 55 procesed and saved.\n",
            "Batch 56 procesed and saved.\n",
            "Batch 57 procesed and saved.\n",
            "Batch 58 procesed and saved.\n",
            "Batch 59 procesed and saved.\n",
            "Batch 60 procesed and saved.\n",
            "Batch 61 procesed and saved.\n",
            "Batch 62 procesed and saved.\n",
            "Batch 63 procesed and saved.\n",
            "Batch 64 procesed and saved.\n",
            "Batch 65 procesed and saved.\n",
            "Batch 66 procesed and saved.\n",
            "Batch 67 procesed and saved.\n",
            "Batch 68 procesed and saved.\n",
            "Batch 69 procesed and saved.\n",
            "Batch 70 procesed and saved.\n",
            "Batch 71 procesed and saved.\n",
            "Batch 72 procesed and saved.\n",
            "Batch 73 procesed and saved.\n",
            "Batch 74 procesed and saved.\n",
            "Batch 75 procesed and saved.\n",
            "Batch 76 procesed and saved.\n",
            "Batch 77 procesed and saved.\n",
            "Batch 78 procesed and saved.\n",
            "Batch 79 procesed and saved.\n",
            "Batch 80 procesed and saved.\n",
            "Batch 81 procesed and saved.\n",
            "Batch 82 procesed and saved.\n",
            "Batch 83 procesed and saved.\n",
            "Batch 84 procesed and saved.\n",
            "Batch 85 procesed and saved.\n",
            "Batch 86 procesed and saved.\n",
            "Batch 87 procesed and saved.\n",
            "Batch 88 procesed and saved.\n",
            "Batch 89 procesed and saved.\n",
            "Batch 90 procesed and saved.\n",
            "Batch 91 procesed and saved.\n",
            "Batch 92 procesed and saved.\n",
            "Batch 93 procesed and saved.\n",
            "Batch 94 procesed and saved.\n",
            "Batch 95 procesed and saved.\n",
            "Batch 96 procesed and saved.\n",
            "Batch 97 procesed and saved.\n",
            "Batch 98 procesed and saved.\n",
            "Batch 99 procesed and saved.\n",
            "Batch 100 procesed and saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ljvb--hbrcK"
      },
      "source": [
        "# Clear the assets folder\n",
        "! rm -r assets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j15GJ2cBpxTO"
      },
      "source": [
        "# !!! Forcefully reset RAM by injecting a list of size 10^10 !!!\n",
        "[1]*10**10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaFqxUFg5jTv"
      },
      "source": [
        "### Save to JSONL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QamFbSfcXJT"
      },
      "source": [
        "import json\n",
        "\n",
        "if not os.path.exists(\"assets\"):\n",
        "        os.makedirs(\"assets\")\n",
        "\n",
        "# with open('assets/TRAIN_DATA.jsonl', 'w') as f:\n",
        "#     for entry in TRAIN_DATA:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/TEST_CONTENT.jsonl', 'w') as f:\n",
        "#     for entry in TEST_CONTENT:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/TEST_CONTEXT.jsonl', 'w') as f:\n",
        "#     for entry in TEST_CONTEXT:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n",
        "\n",
        "# with open('assets/UNSEEN_DATA.jsonl', 'w') as f:\n",
        "#     for entry in UNSEEN_DATA:\n",
        "#         json.dump(entry, f)\n",
        "#         f.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jza6xEbE5ofN"
      },
      "source": [
        "### Zip the JSONL files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsTfX-vqBd3"
      },
      "source": [
        "!zip -r /content/assets.zip /content/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auxp_oSxUe2u"
      },
      "source": [
        "## Extract assets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQf0WHMtsGCy"
      },
      "source": [
        "!unzip /content/assets.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jvWOqtpN9rT"
      },
      "source": [
        "## Convert the data to spaCy's binary format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBf5MoxOC3D"
      },
      "source": [
        "!python -m spacy project run preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xds9LuitNqch"
      },
      "source": [
        "## Check the config file\n",
        "\n",
        "* Cannot check properly with large dataset because of memory issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrMv3h_NgNa"
      },
      "source": [
        "!python -m spacy debug data configs/config.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9jpo3BOOmY"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcH0mZT5OQ_K"
      },
      "source": [
        "# !python -m spacy project run train\n",
        "!python -m spacy train configs/config.cfg --output training/ --paths.train corpus/TEST_CONTEXT.spacy --paths.dev corpus/TEST_CONTENT.spacy --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWXqLUqCWjv"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8_FVJOfCQRx"
      },
      "source": [
        "# !python -m spacy project run evaluate\n",
        "!python -m spacy evaluate training/model-best corpus/fashion_brands_eval.spacy --output training/metrics.json --gpu-id 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYBIJwt5pl2"
      },
      "source": [
        "## Archive the generated model/data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDleMAd8qnS"
      },
      "source": [
        "# !unzip /content/data.zip\n",
        "# !unzip /content/saved_model.zip\n",
        "# !zip -r /content/data.zip /content/data\n",
        "# !zip -r /content/img.zip /content/img\n",
        "# !zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}